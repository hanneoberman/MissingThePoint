
@article{abay08,
  title = {Diagnostics for Multivariate Imputations},
  author = {Abayomi, Kobi and Gelman, Andrew and Levy, Marc},
  date = {2008-06},
  journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  shortjournal = {J Royal Statistical Soc C},
  volume = {57},
  pages = {273--291},
  issn = {0035-9254, 1467-9876},
  doi = {10.1111/j.1467-9876.2007.00613.x},
  url = {http://doi.wiley.com/10.1111/j.1467-9876.2007.00613.x},
  urldate = {2019-09-03},
  abstract = {We consider three sorts of diagnostics for random imputations: displays of the completed data, which are intended to reveal unusual patterns that might suggest problems with the imputations, comparisons of the distributions of observed and imputed data values and checks of the fit of observed data to the model that is used to create the imputations. We formulate these methods in terms of sequential regression multivariate imputation, which is an iterative procedure in which the missing values of each variable are randomly imputed conditionally on all the other variables in the completed data matrix. We also consider a recalibration procedure for sequential regression imputations. We apply these methods to the 2002 environmental sustainability index, which is a linear aggregation of 64 environmental variables on 142 countries.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\YHCKE6CH\\Abayomi e.a. - 2008 - Diagnostics for multivariate imputations.pdf},
  langid = {english},
  number = {3}
}

@online{AcfFunctionDocumentation,
  title = {Acf Function | {{R Documentation}}},
  url = {https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/acf},
  urldate = {2020-04-27},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\GWY8U55V\\acf.html}
}

@book{alli02,
  title = {Missing Data},
  author = {Allison, Paul D.},
  date = {2001},
  publisher = {{Sage publications}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\KHTDX984\\books.html}
}

@article{allisonMultipleImputationMissing2000,
  title = {Multiple {{Imputation}} for {{Missing Data}}: {{A Cautionary Tale}}},
  shorttitle = {Multiple {{Imputation}} for {{Missing Data}}},
  author = {ALLISON, PAUL D.},
  date = {2000-02-01},
  journaltitle = {Sociological Methods \& Research},
  shortjournal = {Sociological Methods \& Research},
  volume = {28},
  pages = {301--309},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124100028003003},
  url = {https://doi.org/10.1177/0049124100028003003},
  urldate = {2020-03-04},
  abstract = {Two algorithms for producing multiple imputations for missing data are evaluated with simulated data. Software using a propensity score classifier with the approximate Bayesian bootstrap produces badly biased estimates of regression coefficients when data on predictor variables are missing at random or missing completely at random. On the other hand, a regression-based method employing the data augmentation algorithm produces estimates with little or no bias.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\J6TQ4ITP\\ALLISON - 2000 - Multiple Imputation for Missing Data A Cautionary.pdf},
  number = {3}
}

@article{bang05,
  title = {Doubly {{Robust Estimation}} in {{Missing Data}} and {{Causal Inference Models}}},
  author = {Bang, Heejung and Robins, James M.},
  date = {2005},
  journaltitle = {Biometrics},
  volume = {61},
  pages = {962--973},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2005.00377.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2005.00377.x},
  urldate = {2019-10-02},
  abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\KJSVWNQL\\Bang en Robins - 2005 - Doubly Robust Estimation in Missing Data and Causa.pdf;C\:\\Users\\4216318\\Zotero\\storage\\MVAKGY8K\\j.1541-0420.2005.00377.html},
  keywords = {Causal inference,Doubly robust estimation,Longitudinal data,Marginal structural model,Missing data,Semiparametrics},
  langid = {english},
  number = {4}
}

@article{bart15,
  title = {Multiple Imputation of Covariates by Fully Conditional Specification: {{Accommodating}} the Substantive Model},
  shorttitle = {Multiple Imputation of Covariates by Fully Conditional Specification},
  author = {Bartlett, Jonathan W and Seaman, Shaun R and White, Ian R and Carpenter, James R},
  date = {2015-08-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {24},
  pages = {462--487},
  issn = {0962-2802},
  doi = {10.1177/0962280214521348},
  url = {https://doi.org/10.1177/0962280214521348},
  urldate = {2019-09-03},
  abstract = {Missing covariate data commonly occur in epidemiological and clinical research, and are often dealt with using multiple imputation. Imputation of partially observed covariates is complicated if the substantive model is non-linear (e.g. Cox proportional hazards model), or contains non-linear (e.g. squared) or interaction terms, and standard software implementations of multiple imputation may impute covariates from models that are incompatible with such substantive models. We show how imputation by fully conditional specification, a popular approach for performing multiple imputation, can be modified so that covariates are imputed from models which are compatible with the substantive model. We investigate through simulation the performance of this proposal, and compare it with existing approaches. Simulation results suggest our proposal gives consistent estimates for a range of common substantive models, including models which contain non-linear covariate effects or interactions, provided data are missing at random and the assumed imputation models are correctly specified and mutually compatible. Stata software implementing the approach is freely available.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\IP6F7U98\\Bartlett e.a. - 2015 - Multiple imputation of covariates by fully conditi.pdf},
  langid = {english},
  number = {4}
}

@book{berryBayesianAnalysisStatistics1996,
  title = {Bayesian {{Analysis}} in {{Statistics}} and {{Econometrics}}: {{Essays}} in {{Honor}} of {{Arnold Zellner}}},
  shorttitle = {Bayesian {{Analysis}} in {{Statistics}} and {{Econometrics}}},
  author = {Berry, Donald A. and Chaloner, Kathryn M. and Geweke, John K. and Zellner, Arnold},
  date = {1996},
  publisher = {{John Wiley \& Sons}},
  abstract = {A definitive work which captures the current state of knowledge in these disciplines and sheds new light on differences and similarities between the Bayesian and Frequentist approaches. Contains a collection of original papers dealing with such topics as inferential matters, the role of prior distributions, regression and related problems, model selection, computational issues, diverse types of applications, and much more.},
  eprint = {oyiYbc_h8S4C},
  eprinttype = {googlebooks},
  isbn = {978-0-471-11856-5},
  keywords = {Business & Economics / Econometrics,Mathematics / Probability & Statistics / Stochastic Processes},
  langid = {english},
  pagetotal = {610}
}

@article{bond16,
  title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models},
  author = {Bondarenko, Irina and Raghunathan, Trivellore},
  date = {2016},
  journaltitle = {Statistics in Medicine},
  volume = {35},
  pages = {3007--3020},
  issn = {1097-0258},
  doi = {10.1002/sim.6926},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6926},
  urldate = {2019-09-27},
  abstract = {Multiple imputation has become a popular approach for analyzing incomplete data. Many software packages are available to multiply impute the missing values and to analyze the resulting completed data sets. However, diagnostic tools to check the validity of the imputations are limited, and the majority of the currently available methods need considerable knowledge of the imputation model. In many practical settings, however, the imputer and the analyst may be different individuals or from different organizations, and the analyst model may or may not be congenial to the model used by the imputer. This article develops and evaluates a set of graphical and numerical diagnostic tools for two practical purposes: (i) for an analyst to determine whether the imputations are reasonable under his/her model assumptions without actually knowing the imputation model assumptions; and (ii) for an imputer to fine tune the imputation model by checking the key characteristics of the observed and imputed values. The tools are based on the numerical and graphical comparisons of the distributions of the observed and imputed values conditional on the propensity of response. The methodology is illustrated using simulated data sets created under a variety of scenarios. The examples focus on continuous and binary variables, but the principles can be used to extend methods for other types of variables. Copyright © 2016 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\BBYIKCGA\\Bondarenko en Raghunathan - 2016 - Graphical and numerical diagnostic tools to assess.pdf;C\:\\Users\\4216318\\Zotero\\storage\\Q53PV2BZ\\sim.html},
  keywords = {congeniality,diagnostics,multiple imputation,propensity score},
  langid = {english},
  number = {17}
}

@incollection{bowmanSmoothingTechniquesVisualisation2008,
  title = {Smoothing {{Techniques}} for {{Visualisation}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Bowman, Adrian W.},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {493--538},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_20},
  url = {https://doi.org/10.1007/978-3-540-33037-0_20},
  urldate = {2020-10-29},
  abstract = {Graphical displays are often constructed to place principal focus on the individual observations in a dataset, and this is particularly helpful in identifying both the typical positions of datapoints and unusual or influential cases. However, in many investigations, principal interest lies in identifying the nature of underlying trends and relationships between variables, and so it is often helpful to enhance graphical displays in wayswhich give deeper insight into these features.This can be very beneficial both for small datasets, where variation can obscure underlying patterns, and large datasets, where the volume of data is so large that effective representation inevitably involves suitable summaries.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\4GE5V6M7\\Bowman - 2008 - Smoothing Techniques for Visualisation.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Great Barrier Reef,Nonparametric Estimate,Nonparametric Regression,Risk Process,Smoothing Technique},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@book{box15,
  title = {Time {{Series Analysis}}: {{Forecasting}} and {{Control}}},
  shorttitle = {Time {{Series Analysis}}},
  author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
  date = {2015-05-29},
  publisher = {{John Wiley \& Sons}},
  abstract = {Praise for the Fourth Edition  "The book follows faithfully the style of the original edition. The approach is heavily motivated by real-world time series, and by developing a complete approach to model building, estimation, forecasting and control."—Mathematical Reviews Bridging classical models and modern topics, the Fifth Edition of Time Series Analysis: Forecasting and Control maintains a balanced presentation of the tools for modeling and analyzing time series. Also describing the latest developments that have occurred in the field over the past decade through applications from areas such as business, finance, and engineering, the Fifth Edition continues to serve as one of the most influential and prominent works on the subject. Time Series Analysis: Forecasting and Control, Fifth Edition provides a clearly written exploration of the key methods for building, classifying, testing, and analyzing stochastic models for time series and describes their use in five important areas of application: forecasting; determining the transfer function of a system; modeling the effects of intervention events; developing multivariate dynamic models; and designing simple control schemes. Along with these classical uses, the new edition covers modern topics with new features that include:  A redesigned chapter on multivariate time series analysis with an expanded treatment of Vector Autoregressive, or VAR models, along with a discussion of the analytical tools needed for modeling vector time series An expanded chapter on special topics covering unit root testing, time-varying volatility models such as ARCH and GARCH, nonlinear time series models, and long memory models Numerous examples drawn from finance, economics, engineering, and other related fields The use of the publicly available R software for graphical illustrations and numerical calculations along with scripts that demonstrate the use of R for model building and forecasting Updates to literature references throughout and new end-of-chapter exercises Streamlined chapter introductions and revisions that update and enhance the exposition  Time Series Analysis: Forecasting and Control, Fifth Edition is a valuable real-world reference for researchers and practitioners in time series analysis, econometrics, finance, and related fields. The book is also an excellent textbook for beginning graduate-level courses in advanced statistics, mathematics, economics, finance, engineering, and physics.},
  eprint = {rNt5CgAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-1-118-67492-5},
  keywords = {Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  langid = {english},
  pagetotal = {709}
}

@article{brandToolkitSASEvaluation2003,
  title = {A Toolkit in {{SAS}} for the Evaluation of Multiple Imputation Methods},
  author = {Brand, Jaap P. L. and van Buuren, Stef and Groothuis‐Oudshoorn, Karin and Gelsema, Edzard S.},
  date = {2003},
  journaltitle = {Statistica Neerlandica},
  volume = {57},
  pages = {36--45},
  issn = {1467-9574},
  doi = {10.1111/1467-9574.00219},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9574.00219},
  urldate = {2019-11-27},
  abstract = {This paper outlines a strategy to validate multiple imputation methods. Rubin's criteria for proper multiple imputation are the point of departure. We describe a simulation method that yields insight into various aspects of bias and efficiency of the imputation process. We propose a new method for creating incomplete data under a general Missing At Random (MAR) mechanism. Software implementing the validation strategy is available as a SAS/IML module. The method is applied to investigate the behavior of polytomous regression imputation for categorical data.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\VSXG993W\\1467-9574.html},
  keywords = {missing data mechanism,multiple imputation,proper imputation,simulation.},
  langid = {english},
  number = {1}
}

@article{brig03,
  title = {Missing.... Presumed at Random: Cost-Analysis of Incomplete Data},
  shorttitle = {Missing.... Presumed at Random},
  author = {Briggs, Andrew and Clark, Taane and Wolstenholme, Jane and Clarke, Philip},
  date = {2003},
  journaltitle = {Health Economics},
  volume = {12},
  pages = {377--392},
  issn = {1099-1050},
  doi = {10.1002/hec.766},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/hec.766},
  urldate = {2019-10-06},
  abstract = {When collecting patient-level resource use data for statistical analysis, for some patients and in some categories of resource use, the required count will not be observed. Although this problem must arise in most reported economic evaluations containing patient-level data, it is rare for authors to detail how the problem was overcome. Statistical packages may default to handling missing data through a so-called ‘complete case analysis’, while some recent cost-analyses have appeared to favour an ‘available case’ approach. Both of these methods are problematic: complete case analysis is inefficient and is likely to be biased; available case analysis, by employing different numbers of observations for each resource use item, generates severe problems for standard statistical inference. Instead we explore imputation methods for generating ‘replacement’ values for missing data that will permit complete case analysis using the whole data set and we illustrate these methods using two data sets that had incomplete resource use information. Copyright © 2002 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\69EUE4JH\\Briggs e.a. - 2003 - Missing.... presumed at random cost-analysis of i.pdf;C\:\\Users\\4216318\\Zotero\\storage\\LBI3Q8QB\\hec.html},
  keywords = {cost-analysis,economic evaluation,missing data},
  langid = {english},
  number = {5}
}

@article{broo00,
  title = {Markov {{Chain Monte Carlo Convergence Assessment}} via {{Two}}-{{Way Analysis}} of {{Variance}}},
  author = {Brooks, S. P. and Giudici, P.},
  date = {2000},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {9},
  pages = {266--285},
  issn = {1061-8600},
  doi = {10.2307/1390654},
  abstract = {In this article we discuss the problem of assessing the performance of Markov chain Monte Carlo (MCMC) algorithms on the basis of simulation output. In essence, we extend the original ideas of Gelman and Rubin and, more recently, Brooks and Gelman, to problems where we are able to split the variation inherent within the MCMC simulation output into two distinct groups. We show how such a diagnostic may be useful in assessing the performance of MCMC samplers addressing model choice problems, such as the reversible jump MCMC algorithm. In the model choice context, we show how the reversible jump MCMC simulation output for parameters that retain a coherent interpretation throughout the simulation, can be used to assess convergence. By considering various decompositions of the sampling variance of this parameter, we can assess the performance of our MCMC sampler in terms of its mixing properties both within and between models and we illustrate our approach in both the graphical Gaussian models and normal mixtures context. Finally, we provide an example of the application of our diagnostic to the assessment of the influence of different starting values on MCMC simulation output, thereby illustrating the wider utility of our method beyond the Bayesian model choice and reversible jump MCMC context.},
  eprint = {1390654},
  eprinttype = {jstor},
  number = {2}
}

@article{broo98,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Brooks, Stephen P. and Gelman, Andrew},
  date = {1998-12-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {7},
  pages = {434--455},
  doi = {10.1080/10618600.1998.10474787},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\Q7I49K25\\Brooks en Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf;C\:\\Users\\4216318\\Zotero\\storage\\SDA8QD6T\\brooksgelman2.pdf;C\:\\Users\\4216318\\Zotero\\storage\\5Q2ITJG5\\10618600.1998.html},
  keywords = {Convergence diagnosis,Inference,Markov chain Monte Carlo},
  number = {4}
}

@article{brooksGeneralMethodsMonitoring1998,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Brooks, Stephen P. and Gelman, Andrew},
  date = {1998-12-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  volume = {7},
  pages = {434--455},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.1998.10474787},
  url = {http://amstat.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787},
  urldate = {2020-03-20},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\HVRQC6HJ\\Brooks en Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf;C\:\\Users\\4216318\\Zotero\\storage\\HCJKPR36\\10618600.1998.html},
  number = {4}
}

@incollection{burneckiVisualizationToolsInsurance2008,
  title = {Visualization {{Tools}} for {{Insurance Risk Processes}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Burnecki, Krzysztof and Weron, Rafał},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {899--920},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_35},
  url = {https://doi.org/10.1007/978-3-540-33037-0_35},
  urldate = {2020-10-29},
  abstract = {This chapter concerns risk processes, which may be the most suitable for computer visualization of all insurance objects. At the same time, risk processes are basic instruments for any non-life actuary – they are needed to calculate the amount of loss that an insurance company may incur. They also appear naturally in rating-triggered step-up bonds, where the interest rate is bound to random changes in company ratings, and catastrophe bonds, where the size of the coupon payment depends on the severity of catastrophic events.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\B4DQA7I4\\Burnecki and Weron - 2008 - Visualization Tools for Insurance Risk Processes.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Claim Size,Excess Function,Probability Plot,Risk Process,Visualization Tool},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{buur06,
  title = {Fully Conditional Specification in Multivariate Imputation},
  author = {Buuren, S. Van and Brand, J. P. L. and Groothuis-Oudshoorn, C. G. M. and Rubin, D. B.},
  date = {2006-12-01},
  journaltitle = {Journal of Statistical Computation and Simulation},
  volume = {76},
  pages = {1049--1064},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/10629360600810434},
  url = {https://doi.org/10.1080/10629360600810434},
  urldate = {2020-03-22},
  abstract = {The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. Attention is given to the statistical behavior under compatible and incompatible models. The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from observed marginal distributions. It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.},
  annotation = {\_eprint: https://doi.org/10.1080/10629360600810434},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\YQ2Q2H3J\\Buuren e.a. - 2006 - Fully conditional specification in multivariate im.pdf;C\:\\Users\\4216318\\Zotero\\storage\\5VUVSNA2\\10629360600810434.html},
  keywords = {Distributional compatibility,Gibbs sampling,Multiple imputation,Multivariate missing data,Proper imputation,Simulation},
  number = {12}
}

@book{buur18,
  title = {Flexible Imputation of Missing Data},
  author = {Van Buuren, Stef},
  date = {2018},
  publisher = {{Chapman and Hall/CRC}}
}

@article{buurenMultipleImputationMissing1999,
  title = {Multiple Imputation of Missing Blood Pressure Covariates in Survival Analysis},
  author = {Buuren, S Van and Boshuizen, H C and Knook, D L},
  date = {1999},
  pages = {14},
  abstract = {This paper studies a non-response problem in survival analysis where the occurrence of missing data in the risk factor is related to mortality. In a study to determine the influence of blood pressure on survival in the very old (85\# years), blood pressure measurements are missing in about 12)5 per cent of the sample. The available data suggest that the process that created the missing data depends jointly on survival and the unknown blood pressure, thereby distorting the relation of interest. Multiple imputation is used to impute missing blood pressure and then analyse the data under a variety of non-response models. One special modelling problem is treated in detail; the construction of a predictive model for drawing imputations if the number of variables is large. Risk estimates for these data appear robust to even large departures from the simplest non-response model, and are similar to those derived under deletion of the incomplete records. Copyright 1999 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\WSSMFDNU\\Multiple imputation - Stat Med 1999.pdf},
  langid = {english}
}

@incollection{changDataVisualizationKernel2008,
  title = {Data {{Visualization}} via {{Kernel Machines}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Chang, Yuan-chin Ivan and Lee, Yuh-Jye and Pao, Hsing-Kuo and Lee, Mei-Hsien and Huang, Su-Yun},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {539--559},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_21},
  url = {https://doi.org/10.1007/978-3-540-33037-0_21},
  urldate = {2020-10-29},
  abstract = {Due to the rapid development of information technology in recent years, it is common to encounter enormousamounts of data collected fromdiverse sources.This has led to a great demand for innovative analytic tools that can handle the kinds of complex data sets that cannot be tackled using traditional statistical methods. Modern data visualization techniques face a similar situation and must also provide adequate solutions.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\AJNN9ZNJ\\Chang et al. - 2008 - Data Visualization via Kernel Machines.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Canonical Correlation Analysis,Data Visualization,Polynomial Kernel,Reproduce Kernel Hilbert Space,Support Vector Regression},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{cookGrandToursProjection2008,
  title = {Grand {{Tours}}, {{Projection Pursuit Guided Tours}}, and {{Manual Controls}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Cook, Dianne and Buja, Andreas and Lee, Eun-Kyung and Wickham, Hadley},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {295--314},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_13},
  url = {https://doi.org/10.1007/978-3-540-33037-0_13},
  urldate = {2020-10-29},
  abstract = {How do we find structure in multidimensional data when computer screens are only two-dimensional? One approach is to project the data onto one or two dimensions. Projections are used in classical statistical methods like principal component analysis (PCA) and linear discriminant analysis. PCA (e.g., Johnson and Wichern 2002) chooses a projection to maximize the variance. Fisher’s linear discriminant (e.g., Johnson and Wichern 2002) chooses a projection that maximizes the relative separation between group means. Projection pursuit (PP) (e.g., Huber 1985) generalizes these ideas into a common strategy, where an arbitrary function on projections is optimized. The scatterplot matrix (e.g., Becker and Cleveland 1987) also can be considered to be a projection method. It shows projections of the data onto all pairs of coordinate axes, the 2-D marginal projections of the data. These projection methods choose a few select projections out of infinitely many.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\DAB825JP\\Cook et al. - 2008 - Grand Tours, Projection Pursuit Guided Tours, and .pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Data Projection,Linear Discriminant Analysis,Manual Control,Multidimensional Data,Projection Pursuit},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{cowl96,
  title = {Markov Chain {{Monte Carlo}} Convergence Diagnostics: A Comparative Review},
  author = {Cowles, Mary Kathryn and Carlin, Bradley P},
  date = {1996},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {91},
  pages = {883--904},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\JIBHQIG4\\CowlesCarlin.1996.pdf},
  number = {434}
}

@incollection{coxMultidimensionalScaling2008,
  title = {Multidimensional {{Scaling}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Cox, Michael A. A. and Cox, Trevor F.},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {315--347},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_14},
  url = {https://doi.org/10.1007/978-3-540-33037-0_14},
  urldate = {2020-10-29},
  abstract = {Suppose dissimilarity data have been collected on a set of n objects or individuals, where there is a value of dissimilarity measured for each pair.The dissimilarity measure used might be a subjective judgement made by a judge, where for example a teacher subjectively scores the strength of friendship between pairs of pupils in her class, or, as an alternative, more objective, measure, she might count the number of contacts made in a day between each pair of pupils. In other situations the dissimilarity measure might be based on a data matrix. The general aim of multidimensional scaling is to find a configuration of points in a space, usually Euclidean, where each point represents one of the objects or individuals, and the distances between pairs of points in the configuration match as well as possible the original dissimilarities between the pairs of objects or individuals. Such configurations can be found using metric and non-metric scaling, which are covered in Sects. 2 and 3. A number of other techniques are covered by the umbrella title of multidimensional scaling (MDS), and here the techniques of Procrustes analysis, unidimensional scaling, individual differences scaling, correspondence analysis and reciprocal averaging are briefly introduced and illustrated with pertinent data sets.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\9AQLUUNL\\Cox and Cox - 2008 - Multidimensional Scaling.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Classical Scaling,Dissimilarity Measure,Multidimensional Scaling,Page Area,Procrustes Analysis},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{elad06,
  title = {Comparison of Methodologies to Assess the Convergence of {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {El Adlouni, Salaheddine and Favre, Anne-Catherine and Bobée, Bernard},
  date = {2006-06-20},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {50},
  pages = {2685--2701},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2005.04.018},
  url = {http://www.sciencedirect.com/science/article/pii/S0167947305000836},
  urldate = {2019-09-20},
  abstract = {One major challenge with the modelization of complex problems using Markov chain Monte Carlo (MCMC) methods is the determination of the length of the chain in order to reach convergence. This paper is devoted to parametric empirical methods testing the stationarity. We compare the methods of Gelman and Rubin, Yu and Mykland, Raftery and Lewis, Geweke, Riemann sums and the subsampling. These methods are tested using three examples: the simple case of the generation of a normal random variable, a bivariate mixture of normal models and a practical case taken from hydrology, namely the shifting level model. Results show that no method works in every case. We therefore suggest a joint use of these techniques. The importance of determining carefully the burn-in period is also highlighted.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\BI6QI44D\\El Adlouni e.a. - 2006 - Comparison of methodologies to assess the converge.pdf;C\:\\Users\\4216318\\Zotero\\storage\\SF4UHRQ2\\S0167947305000836.html},
  keywords = {Burn-in period,Convergence,Gelman and Rubin,Geweke,Gibbs sampler,MCMC,Raftery and Lewis,Riemann sums,Subsampling,Yu and Mykland},
  number = {10}
}

@book{ende10,
  title = {Applied Missing Data Analysis},
  author = {Enders, Craig K},
  date = {2010},
  publisher = {{Guilford press}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\GTM2QHJI\\Enders - 2010 - Applied missing data analysis.pdf},
  isbn = {1-60623-639-3}
}

@article{ende18,
  title = {A Fully Conditional Specification Approach to Multilevel Imputation of Categorical and Continuous Variables.},
  author = {Enders, Craig K. and Keller, Brian T. and Levy, Roy},
  date = {2018-06},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  volume = {23},
  pages = {298--317},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000148},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000148},
  urldate = {2019-09-19},
  abstract = {Specialized imputation routines for multilevel data are widely available in software packages, but these methods are generally not equipped to handle a wide range of complexities that are typical of behavioral science data. In particular, existing imputation schemes differ in their ability to handle random slopes, categorical variables, differential relations at level-1 and level-2, and incomplete level-2 variables. Given the limitations of existing imputation tools, the purpose of this manuscript is to describe a flexible imputation approach that can accommodate a diverse set of two-level analysis problems that includes any of the aforementioned features. The procedure employs a fully conditional specification (also known as chained equations) approach with a latent variable formulation for handling incomplete categorical variables. Computer simulations suggest that the proposed procedure works quite well, with trivial biases in most cases. We provide a software program that implements the imputation strategy, and we use an artificial data set to illustrate its use.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\J2H75436\\enders-keller--levy-2017--.pdf},
  langid = {english},
  number = {2}
}

@article{fichmanMultipleImputationMissing2003,
  title = {Multiple {{Imputation}} for {{Missing Data}}: {{Making}} the Most of {{What}} You {{Know}}},
  shorttitle = {Multiple {{Imputation}} for {{Missing Data}}},
  author = {Fichman, Mark and Cummings, Jonathon N.},
  date = {2003-07-01},
  journaltitle = {Organizational Research Methods},
  shortjournal = {Organizational Research Methods},
  volume = {6},
  pages = {282--308},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10.1177/1094428103255532},
  url = {https://doi.org/10.1177/1094428103255532},
  urldate = {2020-03-04},
  abstract = {Missing data are a common problem in organizational research. Missing data can occur due to attrition in a longitudinal study or nonresponse to questionnaire items in a laboratory or field setting. Improper treatments of missing data (e.g., listwise deletion, mean imputation) can lead to biased statistical inference using complete case analysis statistical techniques. This article presents a simulation and data analysis case study using a method for dealing with missing data, multiple imputation, that allows for valid statistical inference with complete case statistical analysis. Software for implementing multiple imputation under a multivariate normal model is freely and widely available (e.g., NORM, SAS, SOLAS). It should be routinely considered for imputing missing data. The authors illustrate the application of this technique using data from the HomeNet project.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\QTIV6ICE\\Fichman en Cummings - 2003 - Multiple Imputation for Missing Data Making the m.pdf},
  number = {3}
}

@incollection{friendlyBriefHistoryData2008,
  title = {A {{Brief History}} of {{Data Visualization}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Friendly, Michael},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {15--56},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_2},
  url = {https://doi.org/10.1007/978-3-540-33037-0_2},
  urldate = {2020-10-29},
  abstract = {It is common to think of statistical graphics and data visualization as relatively modern developments in statistics. In fact, the graphic representation of quantitative information has deep roots. These roots reach into the histories of the earliestmap making and visual depiction, and later into thematic cartography, statistics and statistical graphics, medicine and other fields. Along the way, developments in technologies (printing, reproduction), mathematical theory and practice, and empirical observation and recording enabled the wider use of graphics and new advances in form and content.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\X3KY4SAG\\Friendly - 2008 - A Brief History of Data Visualization.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {American Statistical Association,Data Visualization,Line Graph,Royal Statistical Society,Statistical Graphic},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@online{FullyConditionalSpecification,
  title = {Fully Conditional Specification in Multivariate Imputation: {{Journal}} of {{Statistical Computation}} and {{Simulation}}: {{Vol}} 76, {{No}} 12},
  url = {https://www-tandfonline-com.proxy.library.uu.nl/doi/full/10.1080/10629360600810434},
  urldate = {2020-03-22},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\F4Y9QAE9\\10629360600810434.html}
}

@book{gelm13,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  date = {2013},
  publisher = {{CRC Press LLC}},
  location = {{Philadelphia, PA, United States}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\ENZRIU2N\\Gelman e.a. - 2013 - Bayesian Data Analysis.pdf;C\:\\Users\\4216318\\Zotero\\storage\\ZSGD97ZP\\reader.html},
  keywords = {Bayesian statistical decision theory.}
}

@article{gelm92,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  author = {Gelman, Andrew and Rubin, Donald B.},
  date = {1992-11},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {7},
  pages = {457--472},
  doi = {10.1214/ss/1177011136},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\CWG9QXQK\\Gelman en Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf;C\:\\Users\\4216318\\Zotero\\storage\\EK2UCSZT\\1177011136.html},
  keywords = {Bayesian inference,convergence of stochastic processes,ECM,EM,Gibbs sampler,importance sampling,Metropolis algorithm,multiple imputation,random-effects model,SIR},
  langid = {english},
  number = {4},
  zmnumber = {06853057}
}

@online{GenerateMissingValues,
  title = {Generate Missing Values with Ampute},
  url = {https://www.gerkovink.com/Amputation_with_Ampute/Vignette/ampute.html},
  urldate = {2020-03-04},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\I7RBXQPI\\ampute.html}
}

@online{GeneratingMissingValues,
  title = {Generating Missing Values for Simulation Purposes: A Multivariate Amputation Procedure: {{Journal}} of {{Statistical Computation}} and {{Simulation}}: {{Vol}} 88, {{No}} 15},
  url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1491577},
  urldate = {2020-03-04},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\J75JCK6X\\00949655.2018.html}
}

@article{gewe92,
  title = {Evaluating the Accuracy of Sampling-Based Approaches to the Calculations of Posterior Moments},
  author = {Geweke, John},
  date = {1992},
  journaltitle = {Bayesian statistics},
  shortjournal = {Bayesian statistics},
  volume = {4},
  pages = {641--649}
}

@book{gilk95,
  title = {Markov {{Chain Monte Carlo}} in {{Practice}}},
  author = {Gilks, W. R. and Richardson, S. and Spiegelhalter, David},
  date = {1995-12-01},
  publisher = {{CRC Press}},
  abstract = {In a family study of breast cancer, epidemiologists in Southern California increase the power for detecting a gene-environment interaction. In Gambia, a study helps a vaccination program reduce the incidence of Hepatitis B carriage. Archaeologists in Austria place a Bronze Age site in its true temporal location on the calendar scale. And in France, researchers map a rare disease with relatively little variation.Each of these studies applied Markov chain Monte Carlo methods to produce more accurate and inclusive results. General state-space Markov chain theory has seen several developments that have made it both more accessible and more powerful to the general statistician. Markov Chain Monte Carlo in Practice introduces MCMC methods and their applications, providing some theoretical background as well. The authors are researchers who have made key contributions in the recent development of MCMC methodology and its application. Considering the broad audience, the editors emphasize practice rather than theory, keeping the technical content to a minimum. The examples range from the simplest application, Gibbs sampling, to more complex applications. The first chapter contains enough information to allow the reader to start applying MCMC in a basic way. The following chapters cover main issues, important concepts and results, techniques for implementing MCMC, improving its performance, assessing model adequacy, choosing between models, and applications and their domains.Markov Chain Monte Carlo in Practice is a thorough, clear introduction to the methodology and applications of this simple idea with enormous potential. It shows the importance of MCMC in real applications, such as archaeology, astronomy, biostatistics, genetics, epidemiology, and image analysis, and provides an excellent base for MCMC to be applied to other fields as well.},
  eprint = {TRXrMWY_i2IC},
  eprinttype = {googlebooks},
  isbn = {978-0-412-05551-5},
  keywords = {Mathematics / Probability & Statistics / General,Science / Life Sciences / Biology},
  langid = {english},
  pagetotal = {522}
}

@book{grah12,
  title = {Missing Data: {{Analysis}} and Design},
  author = {Graham, John W},
  date = {2012},
  publisher = {{Springer Science \& Business Media}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\688F2XST\\Graham - 2012 - Missing data Analysis and design.pdf},
  isbn = {1-4614-4018-1}
}

@incollection{hardleGraphicalDataRepresentation2008,
  title = {Graphical {{Data Representation}} in {{Bankruptcy Analysis}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Härdle, Wolfgang and Moro, Rouslan A. and Schäfer, Dorothea},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {853--872},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_33},
  url = {https://doi.org/10.1007/978-3-540-33037-0_33},
  urldate = {2020-10-29},
  abstract = {Graphical data representation is an important model selection tool in bankruptcy analysis, since this problem is highly nonlinear and its numerical representation is not very transparent. In classical rating models, the convenient representation of the ratings in a closed form reduces the need for graphical tools. In contrast to this, more accurate nonlinear nonparametric models often rely on visualisation. We demonstrate the utilisation of visualisation techniques at different stages of corporate default analysis, which is based on the application of support vector machines (SVM). These stages are the selection of variables (predictors), probability of default (PD) estimation, and the representation of PDs for two- and higher dimensional models with colour coding.The selection of a proper colour scheme becomes crucial to the correct visualisation of PDs at this stage.The mapping of scores into PDs is done as a nonparametric regression with monotonisation. The SVM learns a nonparametric score function that is, in turn, nonparametrically transformed into PDs. Since PDs cannot be represented in a closed form, other ways of displaying themmust be found. Graphical tools make this possible.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\9PJ7PXCC\\Härdle et al. - 2008 - Graphical Data Representation in Bankruptcy Analys.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {European Central Bank,Graphical Tool,Linear Support Vector Machine,Support Vector Machine,Support Vector Machine Model},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{harelEstimationAdjustedIncomplete2009,
  title = {The Estimation of {{R}} 2 and Adjusted {{R}} 2 in Incomplete Data Sets Using Multiple Imputation},
  author = {Harel, Ofer},
  date = {2009-10-01},
  journaltitle = {Journal of Applied Statistics},
  volume = {36},
  pages = {1109--1118},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763},
  doi = {10.1080/02664760802553000},
  url = {https://doi.org/10.1080/02664760802553000},
  urldate = {2020-03-16},
  abstract = {The coefficient of determination, known also as the R 2, is a common measure in regression analysis. Many scientists use the R 2 and the adjusted R 2 on a regular basis. In most cases, the researchers treat the coefficient of determination as an index of ‘usefulness’ or ‘goodness of fit,’ and in some cases, they even treat it as a model selection tool. In cases in which the data is incomplete, most researchers and common statistical software will use complete case analysis in order to estimate the R 2, a procedure that might lead to biased results. In this paper, I introduce the use of multiple imputation for the estimation of R 2 and adjusted R 2 in incomplete data sets. I illustrate my methodology using a biomedical example.},
  annotation = {\_eprint: https://doi.org/10.1080/02664760802553000},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\XKMGHWUF\\Harel - 2009 - The estimation of R 2 and adjusted R 2 in incomple.pdf;C\:\\Users\\4216318\\Zotero\\storage\\9GZA2JEG\\02664760802553000.html},
  keywords = {coefficient of determination,incomplete data,linear regression,multiple imputation},
  number = {10}
}

@incollection{heibergerStructuredSetsGraphs2008,
  title = {Structured {{Sets}} of {{Graphs}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Heiberger, Richard M. and Holland, Burt},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {415--445},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_17},
  url = {https://doi.org/10.1007/978-3-540-33037-0_17},
  urldate = {2020-10-29},
  abstract = {We present many examples of structured sets of graphs that convey and support statistical analyses. Structured sets of graphs can be drawn with any modern statistical software system with graphics capabilities. We use S-Plus and R, two dialects of the S language that offer substantial capabilities for producing graphs customized to the particular needs and visions of the analyst. We emphasize two basic paradigms for constructing structured graphs: Cartesian products and the Trellis paradigm. Our software for all examples in this article is available from Heiberger and Holland (2004).},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\AAMWDPQQ\\Heiberger and Holland - 2008 - Structured Sets of Graphs.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Individual Panel,Interaction Plot,Label Correlation Coefficient,Pain Category,Pelvic Incidence},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@book{hoff09,
  title = {A {{First Course}} in {{Bayesian Statistical Methods}}},
  author = {Hoff, Peter D.},
  date = {2009},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-92407-6},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\YPEBD5NA\\Hoff - 2009 - A First Course in Bayesian Statistical Methods.pdf},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@incollection{hofmannMosaicPlotsTheir2008,
  title = {Mosaic {{Plots}} and {{Their Variants}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Hofmann, Heike},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {617--642},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_24},
  url = {https://doi.org/10.1007/978-3-540-33037-0_24},
  urldate = {2020-10-29},
  abstract = {In this chapter we consider mosaicplots, which were introduced by Hartigan and Kleiner (1981) as a way of visualizing contingency tables. Named “mosaicplots” due to their resemblance to the art form, they consist of groups of rectangles that represent the cells in a contingency table. Both the sizes and the positions of the rectangles are relevant to mosaicplot interpretation, making them one of the more advanced plots around.With a little practice they can become an invaluable tool in the representation and exploration of multivariate categorical data.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\25223IZF\\Hofmann - 2008 - Mosaic Plots and Their Variants.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Contingency Table,Intelligence Quotient,Loglinear Model,Parental Encouragement,Purity Function},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{hortonMultipleImputationPractice2001,
  title = {Multiple {{Imputation}} in {{Practice}}: {{Comparison}} of {{Software Packages}} for {{Regression Models With Missing Variables}}},
  shorttitle = {Multiple {{Imputation}} in {{Practice}}},
  author = {Horton, Nicholas J and Lipsitz, Stuart R},
  date = {2001-08},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {55},
  pages = {244--254},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/000313001317098266},
  url = {http://www.tandfonline.com/doi/abs/10.1198/000313001317098266},
  urldate = {2020-03-04},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\RCIXUG22\\Horton en Lipsitz - 2001 - Multiple Imputation in Practice Comparison of Sof.pdf},
  langid = {english},
  number = {3}
}

@book{hynd18,
  title = {Forecasting: Principles and Practice},
  shorttitle = {Forecasting},
  author = {Hyndman, Rob J. and Athanasopoulos, George},
  date = {2018},
  publisher = {{OTexts}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\NISHZF9I\\Hyndman en Athanasopoulos - 2018 - Forecasting principles and practice.pdf;C\:\\Users\\4216318\\Zotero\\storage\\LTZRLHJ6\\books.html}
}

@incollection{inselbergParallelCoordinatesVisualization2008,
  title = {Parallel {{Coordinates}}: {{Visualization}}, {{Exploration}} and {{Classification}} of {{High}}-{{Dimensional Data}}},
  shorttitle = {Parallel {{Coordinates}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Inselberg, Alfred},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {643--680},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_25},
  url = {https://doi.org/10.1007/978-3-540-33037-0_25},
  urldate = {2020-10-29},
  abstract = {A dataset with M items has 2M subsets, any one of which may be the one we really want. With a good data display, our own fantastic pattern-recognition abilities can not only sort through this combinatorial explosion, but they can also extract insights fromthe visual patterns. These are the core reasons for data visualization. With parallel coordinates (abbrev. f-coords), the search for multivariate relations in highdimensional datasets is transformed into a 2-D pattern recognition problem. In this chapter, the guidelines and strategy for knowledge discovery using parallel coordinates are illustrated on various real datasets, one with 400 variables froma manufacturing process. A geometric classification algorithm based on f-coords is presented and applied to complex datasets. It has low computational complexity, providing the classification rule explicitly and visually.Theminimal set of variables required to state the rule are found and ordered by their predictive value. A visual economic model of a real country is constructed and analyzed to illustrate how multivariate relations can be modeled using hypersurfaces.The overview at the end provides a basic summary of f-coords and a prelude of what is on the way: the distillation of relational information into patterns that eliminate need for polygonal lines altogether.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\SVGPB9CE\\Inselberg - 2008 - Parallel Coordinates Visualization, Exploration a.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Exploratory Data Analysis,Hamiltonian Path,IEEE Conf,Nest Cavity,Polygonal Line},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{jankVisualizingFunctionalData2008,
  title = {Visualizing {{Functional Data}} with an {{Application}} to {{eBay}}’s {{Online Auctions}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Jank, Wolfgang and Shmueli, Galit and Plaisant, Catherine and Shneiderman, Ben},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {873--898},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_34},
  url = {https://doi.org/10.1007/978-3-540-33037-0_34},
  urldate = {2020-10-29},
  abstract = {Technological advances in the measurement, collection, and storage of data have led tomore andmore complex data structures. Examples of such structures include measurements of the behavior of individuals over time, digitized two- or three-dimensional images of the brain, and recordings of three- or even four-dimensional movements of objects traveling through space and time. Such data, although recorded in a discrete fashion, are usually thought of as continuous objects that are represented by functional relationships. This gave rise to functional data analysis (FDA), which was made popular by the monographs of Ramsay and Silverman (1997, 2002), where the center of interest is a set of curves, shapes, objects, or, more generally, a set of functional observations, in contrast to classical statistics where interest centers on a set of data vectors. In that sense, functional data is not only different from the data structure studied in classical statistics, but it actually generalizes it. Many of these new data structures require new statistical methods to unveil the information that they carry.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\PY7LACE5\\Jank et al. - 2008 - Visualizing Functional Data with an Application to.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Auction Price,Functional Data,Functional Data Analysis,Information Visualization,Online Auction},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{kermanVisualizationBayesianData2008,
  title = {Visualization in {{Bayesian Data Analysis}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Kerman, Jouni and Gelman, Andrew and Zheng, Tian and Ding, Yuejing},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {709--724},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_27},
  url = {https://doi.org/10.1007/978-3-540-33037-0_27},
  urldate = {2020-10-29},
  abstract = {Modern Bayesian statistical science commonly proceeds without reference to statistical graphics; both involve computation, but they are rarely considered to be connected. Traditional views about the usage of Bayesian statistics and statistical graphics result in a certain clash of attitudes between the two. Bayesians might do some exploratory data analysis (EDA) to start with, but once the model or class of models is specified, the next analytical step is to fit the data; graphs are then typically used to check convergence of simulations, or they are used as teaching aids or as presentation tools – but not as part of the data analysis. Exploratory data analysis appears to have no formal place in Bayesian statistics once amodel has actually been fitted.According to this extreme view, the only connection between Bayesian inference and graphics occurs through convergence plots ofMarkov chain simulations, and histograms and kernel density plots of the resulting estimates of scalar parameters.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\P6RTEDRA\\Kerman et al. - 2008 - Visualization in Bayesian Data Analysis.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Bayesian Inference,Exploratory Data Analysis,Model Check,Reference Distribution,Social Network Size},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@report{lace07,
  title = {Sequential Regression Multiple Imputation for Incomplete Multivariate Data Using {{Markov}} Chain {{Monte Carlo}}},
  author = {Lacerda, Miguel and Ardington, Cally and Leibbrandt, Murray},
  date = {2007},
  institution = {{University of Cape Town, South Africa}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\NIKK7HSY\\2008_13.pdf}
}

@online{lambertRobustMCMCConvergence2020,
  title = {\${{R}}\^*\$: {{A}} Robust {{MCMC}} Convergence Diagnostic with Uncertainty Using Gradient-Boosted Machines},
  shorttitle = {\${{R}}\^*\$},
  author = {Lambert, Ben and Vehtari, Aki},
  date = {2020-09-01},
  url = {http://arxiv.org/abs/2003.07900},
  urldate = {2020-09-03},
  abstract = {Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over the past three decades: mainly because of this, Bayesian inference is now a workhorse of applied scientists. Under general conditions, MCMC sampling converges asymptotically to the posterior distribution, but this provides no guarantees about its finite sample performance. The predominant method for monitoring convergence is to run multiple chains and monitor individual chains' characteristics and compare these to the population as a whole: if within-chain and between-chain summaries are comparable, then this is taken to indicate that the chains have converged to a common stationary distribution. Here, we introduce a new method for diagnosing convergence based on whether a machine learning classifier model can successfully discriminate the individual chains. We call this convergence measure \$R\^*\$. In contrast to the predominant \$\textbackslash widehat\{R\}\$, \$R\^*\$ is a single statistic across all parameters that indicates lack of mixing, although individual variables' importance for this metric can also be determined. Additionally, \$R\^*\$ is not based on any single characteristic of the sampling distribution; instead using all the information in the chain, including that given by the joint sampling distribution, which is currently largely overlooked by existing approaches. Since our choice of machine learning classifier, a gradient-boosted regression trees model (GBM), provides uncertainty in predictions, as a byproduct, we obtain uncertainty in \$R\^*\$. The method is straightforward to implement, robust to GBM hyperparameter choice, and could be a complementary additional check on MCMC convergence for applied analyses.},
  archivePrefix = {arXiv},
  eprint = {2003.07900},
  eprinttype = {arxiv},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\25RYXYQ6\\Lambert en Vehtari - 2020 - $R^$ A robust MCMC convergence diagnostic with u.pdf;C\:\\Users\\4216318\\Zotero\\storage\\JTMX9LDE\\2003.html},
  keywords = {Statistics - Applications,Statistics - Methodology},
  primaryClass = {stat}
}

@incollection{leischVisualizingClusterAnalysis2008,
  title = {Visualizing {{Cluster Analysis}} and {{Finite Mixture Models}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Leisch, Friedrich},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {561--587},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_22},
  url = {https://doi.org/10.1007/978-3-540-33037-0_22},
  urldate = {2020-10-29},
  abstract = {Data visualization can greatly enhance our understanding of multivariate data structures, and so it is no surprise that cluster analysis and data visualization often go hand in hand, and that textbooks like Gordon (1999) or Everitt et al. (2001) are full of figures. In particular, hierarchical cluster analysis is almost always accompanied by a dendrogram. Results frompartitioning cluster analysis can be visualized by projecting the data into two-dimensional space or using parallel coordinates. Cluster membership is usually represented by different colors and glyphs, or by dividing clusters into several panels of a trellis display (Becker et al., 1996). In addition, silhouette plots (Rousseeuw, 1987) provide a popular tool for diagnosing the quality of a partition. Some of the popularity of self-organizing feature maps (Kohonen, 1989) with practitioners in various fields can be explained by the fact that the results can be “easily” visualized.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\YIH5VK4I\\Leisch - 2008 - Visualizing Cluster Analysis and Finite Mixture Mo.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Cluster Centroid,Cluster Membership,Electoral District,Finite Mixture Model,Manhattan Distance},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{li14,
  title = {Multiple {{Imputation}} by {{Ordered Monotone Blocks With Application}} to the {{Anthrax Vaccine Research Program}}},
  author = {Li, Fan and Baccini, Michela and Mealli, Fabrizia and Zell, Elizabeth R. and Frangakis, Constantine E. and Rubin, Donald B.},
  date = {2014-07-03},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  volume = {23},
  pages = {877--892},
  issn = {1061-8600},
  doi = {10.1080/10618600.2013.826583},
  url = {https://www.tandfonline.com/doi/10.1080/10618600.2013.826583},
  urldate = {2019-09-10},
  abstract = {Multiple imputation (MI) has become a standard statistical technique for dealing with missing values. The CDC Anthrax Vaccine Research Program (AVRP) dataset created new challenges for MI due to the large number of variables of different types and the limited sample size. A common method for imputing missing data in such complex studies is to specify, for each of J variables with missing values, a univariate conditional distribution given all other variables, and then to draw imputations by iterating over the J conditional distributions. Such fully conditional imputation strategies have the theoretical drawback that the conditional distributions may be incompatible. When the missingness pattern is monotone, a theoretically valid approach is to specify, for each variable with missing values, a conditional distribution given the variables with fewer or the same number of missing values and sequentially draw from these distributions. In this article, we propose the “multiple imputation by ordered monotone blocks” approach, which combines these two basic approaches by decomposing any missingness pattern into a collection of smaller “constructed” monotone missingness patterns, and iterating. We apply this strategy to impute the missing data in the AVRP interim data. Supplemental materials, including all source code and a synthetic example dataset, are available online.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\Z2SXCLDC\\Li e.a. - 2014 - Multiple Imputation by Ordered Monotone Blocks Wit.pdf;C\:\\Users\\4216318\\Zotero\\storage\\PUZN3HTY\\10618600.2013.html},
  number = {3}
}

@article{li91,
  title = {Significance Levels from Repeated P-Values with Multiply-Imputed Data},
  author = {Li, Kim-Hung and Meng, Xiao-Li and Raghunathan, Trivellore E and Rubin, Donald B},
  date = {1991},
  journaltitle = {Statistica Sinica},
  shortjournal = {Statistica Sinica},
  pages = {65--92},
  issn = {1017-0405},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\3KXYTRMF\\Li Meng Raghunathan Rubin - combining p values.pdf}
}

@article{liu14,
  title = {On the Stationary Distribution of Iterative Imputations},
  author = {Liu, J. and Gelman, A. and Hill, J. and Su, Y.-S. and Kropko, J.},
  date = {2014-03-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {101},
  pages = {155--173},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/ast044},
  url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/ast044},
  urldate = {2019-10-21},
  abstract = {Iterative imputation, in which variables are imputed one at a time conditional on all the oth- 15 ers, is a popular technique that can be convenient and flexible, as it replaces a potentially difficult multivariate modeling problem with relatively simple univariate regressions. In this paper, we begin to characterize the stationary distributions of iterative imputations and their statistical properties, accounting for the conditional models being iteratively estimated from data rather than being pre-specified. When the families of conditional models are compatible, we provide 20 sufficient conditions under which the imputation distribution converges in total variation to the posterior distribution of a Bayesian model. When the conditional models are incompatible but valid, we show that the combined imputation estimator is consistent.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\5BFL93ME\\Liu e.a. - 2014 - On the stationary distribution of iterative imputa.pdf},
  langid = {english},
  number = {1}
}

@article{liuStationaryDistributionIterative,
  title = {On the {{Stationary Distribution}} of {{Iterative Imputations}}},
  author = {Liu, Jingchen and Gelman, Andrew},
  pages = {18},
  abstract = {Iterative imputation, in which variables are imputed one at a time conditional on all the oth- 15 ers, is a popular technique that can be convenient and flexible, as it replaces a potentially difficult multivariate modeling problem with relatively simple univariate regressions. In this paper, we begin to characterize the stationary distributions of iterative imputations and their statistical properties, accounting for the conditional models being iteratively estimated from data rather than being pre-specified. When the families of conditional models are compatible, we provide 20 sufficient conditions under which the imputation distribution converges in total variation to the posterior distribution of a Bayesian model. When the conditional models are incompatible but valid, we show that the combined imputation estimator is consistent.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\QCDVMXA6\\Liu and Gelman - On the Stationary Distribution of Iterative Imputa.pdf},
  langid = {english}
}

@article{liuStationaryDistributionIterative2014,
  title = {On the Stationary Distribution of Iterative Imputations},
  author = {LIU, JINGCHEN and GELMAN, ANDREW and HILL, JENNIFER and SU, YU-SUNG and KROPKO, JONATHAN},
  date = {2014},
  journaltitle = {Biometrika},
  volume = {101},
  pages = {155--173},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  abstract = {Iterative imputation, in which variables are imputed one at a time conditional on all the others, is a popular technique that can be convenient and flexible, as it replaces a potentially difficult multivariate modelling problem with relatively simple univariate regressions. In this paper, we begin to characterize the stationary distributions of iterative imputations and their statistical properties, accounting for the conditional models being iteratively estimated from data rather than being prespecified. When the families of conditional models are compatible, we provide sufficient conditions under which the imputation distribution converges in total variation to the posterior distribution of a Bayesian model. When the conditional models are incompatible but valid, we show that the combined imputation estimator is consistent.},
  eprint = {43305601},
  eprinttype = {jstor},
  number = {1}
}

@incollection{lohRegressionPartsFitting2008,
  title = {Regression by {{Parts}}: {{Fitting Visually Interpretable Models}} with {{GUIDE}}},
  shorttitle = {Regression by {{Parts}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Loh, Wei-Yin},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {447--469},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_18},
  url = {https://doi.org/10.1007/978-3-540-33037-0_18},
  urldate = {2020-10-29},
  abstract = {Regression modeling often requires many subjective decisions, such as choice of transformation for each variable and the type and number of terms to include in the model. The transformations may be as simple as powers and cross-products or as sophisticated as indicator functions and splines. Sometimes, the transformations are chosen to satisfy certain subjective criteria such as approximate normality of the marginal distributions of the predictor variables. Further, model building is almost always an iterative process, with the fit of the model evaluated each time terms are added or deleted.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\ZUD8LRLQ\\Loh - 2008 - Regression by Parts Fitting Visually Interpretabl.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Crash Test,Interpretable Model,Inverse Regression,Leaf Node,Simple Linear Model},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{luReconstructionVisualizationAnalysis2008,
  title = {Reconstruction, {{Visualization}} and {{Analysis}} of {{Medical Images}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Lu, Henry Horng-Shing},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {813--830},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_31},
  url = {https://doi.org/10.1007/978-3-540-33037-0_31},
  urldate = {2020-10-29},
  abstract = {Advances in medical imaging systems havemade significant contributions to medical diagnoses and treatments by providing anatomic and functional information about human bodies that is difficult to obtain without these techniques. These modalities also generate large quantities of noisy data that need modern techniques of computational statistics for image reconstruction, visualization and analysis. This article will report recent research in this area and suggest challenges that will need to be addressed by future studies. Specifically, I will discuss computational statistics for positron emission tomography, ultrasound images and magnetic resonance images from the perspectives of image reconstruction, image segmentation and vision model-based image analysis.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\R53WTVFP\\Lu - 2008 - Reconstruction, Visualization and Analysis of Medi.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Active Contour Model,Feature Vector,Photon Pair,Positron Emission Tomography,Ultrasound Image},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@book{lync07,
  title = {Introduction to Applied {{Bayesian}} Statistics and Estimation for Social Scientists},
  author = {Lynch, Scott M.},
  date = {2007},
  publisher = {{Springer Science \& Business Media}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\GD5UTHAC\\books.html}
}

@book{mack03,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David JC and Mac Kay, David JC},
  date = {2003},
  publisher = {{Cambridge university press}}
}

@article{meng94,
  title = {Multiple-{{Imputation Inferences}} with {{Uncongenial Sources}} of {{Input}}},
  author = {Meng, Xiao-Li},
  date = {1994-11},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {9},
  pages = {538--558},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177010269},
  url = {https://projecteuclid.org/euclid.ss/1177010269},
  urldate = {2019-10-03},
  abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\PHA35LD7\\Meng - 1994 - Multiple-Imputation Inferences with Uncongenial So.pdf;C\:\\Users\\4216318\\Zotero\\storage\\MNNUQRPL\\1177010269.html},
  keywords = {Congeniality,importance sampling,incomplete data,missing data,nonresponse,normalizing constants,public-use data file,randomization,self-efficiency},
  langid = {english},
  number = {4}
}

@incollection{meyerVisualizingContingencyTables2008,
  title = {Visualizing {{Contingency Tables}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Meyer, David and Zeileis, Achim and Hornik, Kurt},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {589--616},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_23},
  url = {https://doi.org/10.1007/978-3-540-33037-0_23},
  urldate = {2020-10-29},
  abstract = {Categorical data analysis is typically based on two- or higher dimensional contingency tables, cross-tabulating the co-occurrences of levels of nominal and/or ordinal data. In order to explain these, statisticians typically look for (conditional) independence structures using common methods such as independence tests and log-linear models. One idea behind the use of visualization techniques is to use the human visual system to detect structures in the data that may not be obvious fromsolely numeric output (e.g., test statistics). Whether the task is purely exploratory or modelbased, techniques such as mosaic, sieve, and association plots offer good support for visualization. Mosaic and sieve plots in particular have been extended over the last two decades, and implementations exist in many statistical environments.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\FPSMSM4J\\Meyer et al. - 2008 - Visualizing Contingency Tables.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Conditional Independence Model,Mosaic Plot,Pair Plot,Pearson Residual,Visit Frequency},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{mice,
  title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
  shorttitle = {Mice},
  author = {Van Buuren, Stef and Groothuis-Oudshoorn, Karin},
  date = {2011-12-12},
  journaltitle = {Journal of Statistical Software},
  volume = {45},
  pages = {1--67},
  doi = {10.18637/jss.v045.i03},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\W7DXJIVB\\Buuren en Groothuis-Oudshoorn - 2011 - mice Multivariate Imputation by Chained Equations.pdf;C\:\\Users\\4216318\\Zotero\\storage\\6GARXLG6\\v045i03.html},
  langid = {english},
  number = {1}
}

@incollection{michailidisDataVisualizationTheir2008,
  title = {Data {{Visualization Through Their Graph Representations}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Michailidis, George},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {103--120},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_5},
  url = {https://doi.org/10.1007/978-3-540-33037-0_5},
  urldate = {2020-10-29},
  abstract = {The amount of data and information collected and retained by organizations and businesses is constantly increasing, due to advances in data collection, computerization of transactions, and breakthroughs in storage technology. Further, many attributes are also recorded, resulting in very high-dimensional data sets. Typically, the applications involve large-scale information banks, such as data warehouses that contain interrelated data from a number of sources. Examples of new technologies giving rise to large, high-dimensional data sets are high-throughput genomic and proteomic technologies, sensor-based monitoring systems, etc. Finally, new application areas such as biochemical pathways, web documents, etc. produce data with inherent structure that cannot be simply captured by numbers.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\YUXYCXI6\\Michailidis - 2008 - Data Visualization Through Their Graph Representat.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Comput Stat Data Anal,Data Visualization,Graph Drawing,Multiple Correspondence Analysis,Weighted Graph},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{minnotteMultivariateVisualizationDensity2008,
  title = {Multivariate {{Visualization}} by {{Density Estimation}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Minnotte, Michael C. and Sain, Stephan R. and Scott, DavidW.},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {389--413},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_16},
  url = {https://doi.org/10.1007/978-3-540-33037-0_16},
  urldate = {2020-10-29},
  abstract = {Density estimation and related methods provide a powerful set of tools for visualization of data-based distributions in one, two, and higher dimensions. This chapter examines a variety of such estimators, as well as the various issues related to their theoretical quality and practical application.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\J3CXHFGN\\Minnotte et al. - 2008 - Multivariate Visualization by Density Estimation.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Kernel Estimate,Mean Integrate Square Error,Normal Kernel,Stat Assoc,Toxic Release Inventory},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{morr19,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  date = {2019-05-20},
  journaltitle = {Statistics in Medicine},
  volume = {38},
  pages = {2074--2102},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1097-0258},
  doi = {10.1002/sim.8086},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/sim.8086},
  urldate = {2020-04-16},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo‐random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods...},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\C2X36276\\Morris e.a. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\4216318\\Zotero\\storage\\LN3P265W\\login.html},
  langid = {english},
  number = {11}
}

@article{murr18,
  title = {Multiple {{Imputation}}: {{A Review}} of {{Practical}} and {{Theoretical Findings}}},
  shorttitle = {Multiple {{Imputation}}},
  author = {Murray, Jared S.},
  date = {2018-05},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {33},
  pages = {142--159},
  doi = {10.1214/18-STS644},
  abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in flexible joint modeling and sequential regression/chained equations/fully conditional specification approaches. Finally, we compare and contrast different methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\TT9QUVIH\\Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf},
  langid = {english},
  number = {2}
}

@incollection{murrellStaticGraphics2008,
  title = {Static {{Graphics}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Murrell, Paul},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {79--101},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_4},
  url = {https://doi.org/10.1007/978-3-540-33037-0_4},
  urldate = {2020-10-29},
  abstract = {This chapter describes the requirements for a modern statistical graphics system for the production of static plots. There is a discussion of the production of complete plots, customizing plots, adding extra output to plots and creating entirely new plots. Statistical graphics is described as an extension of a general graphics language. There is an emphasis on the importance of support for sophisticated graphics facilities such as semitransparent colours, image compositing operators and the complex arrangement of graphical elements.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\SPVNYQWZ\\Murrell - 2008 - Static Graphics.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Data Symbol,Graphical User Interface,Grid Line,Static Display,Statistical Graphic},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@book{mvtnorm,
  title = {Computation of Multivariate Normal and t Probabilities},
  author = {Genz, Alan and Bretz, Frank},
  date = {2009},
  publisher = {{Springer-Verlag}},
  location = {{Heidelberg}},
  isbn = {978-3-642-01688-2},
  series = {Lecture Notes in Statistics}
}

@incollection{nakanoProgrammingStatisticalData2008,
  title = {Programming {{Statistical Data Visualization}} in the {{Java Language}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Nakano, Junji and Yamamoto, Yoshikazu and Honda, Keisuke},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {725--756},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_28},
  url = {https://doi.org/10.1007/978-3-540-33037-0_28},
  urldate = {2020-10-29},
  abstract = {When we have to write programs for statistical data visualization using a generalpurpose programming language, for example, to achieve new functions or extensibility, the Java language is an appropriate choice. The object-oriented characteristics of Java are suitable for building graphical and interactive programs. Java has well-prepared standard graphical libraries that reduce our programming tasks and increase the portability of software to different platforms. Furthermore, recently developed solutions for object-oriented programming – so called “design patterns” – are useful for building statistical data visualization programs.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\7Y9Y7ZYR\\Nakano et al. - 2008 - Programming Statistical Data Visualization in the .pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Class Diagram,Design Pattern,Graphic Library,Mouse Button,Statistical Graphic},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{neym34,
  title = {On the {{Two Different Aspects}} of the {{Representative Method}}: {{The Method}} of {{Stratified Sampling}} and the {{Method}} of {{Purposive Selection}}},
  author = {Neyman, Jerzy},
  date = {1934},
  journaltitle = {Journal of the Royal Statistical Society},
  volume = {97},
  pages = {558--625},
  doi = {10.2307/2342192},
  number = {4}
}

@article{nguy17,
  title = {Model Checking in Multiple Imputation: An Overview and Case Study},
  shorttitle = {Model Checking in Multiple Imputation},
  author = {Nguyen, Cattram D. and Carlin, John B. and Lee, Katherine J.},
  date = {2017-08-23},
  journaltitle = {Emerging Themes in Epidemiology},
  shortjournal = {Emerging Themes in Epidemiology},
  volume = {14},
  pages = {8},
  issn = {1742-7622},
  doi = {10.1186/s12982-017-0062-6},
  url = {https://doi.org/10.1186/s12982-017-0062-6},
  urldate = {2019-09-27},
  abstract = {Multiple imputation has become very popular as a general-purpose method for handling missing data. The validity of multiple-imputation-based analyses relies on the use of an appropriate model to impute the missing values. Despite the widespread use of multiple imputation, there are few guidelines available for checking imputation models.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\GNUQEAQA\\Nguyen e.a. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\CDXNKIZY\\s12982-017-0062-6.html},
  number = {1}
}

@incollection{palumboHugeMultidimensionalData2008,
  title = {Huge {{Multidimensional Data Visualization}}: {{Back}} to the {{Virtue}} of {{Principal Coordinates}} and {{Dendrograms}} in the {{New Computer Age}}},
  shorttitle = {Huge {{Multidimensional Data Visualization}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Palumbo, Francesco and Vistocco, Domenico and Morineau, Alain},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {349--387},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_15},
  url = {https://doi.org/10.1007/978-3-540-33037-0_15},
  urldate = {2020-10-29},
  abstract = {Many papers refer to Tukey’s (1977) treatise on exploratory data analysis as the contribution that transformed statistical thinking. In actual fact, new ideas introduced by Tukey prompted many statisticians to give a more prominent role to data visualization and more generally to data. However, J.W. Tukey in 1962 had already begun his daring provocation when at the annual meeting of the Institute of Mathematical Statistics he gave his talk entitled “The Future of Data Analysis” (Tukey, 1962).},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\FDFS46QK\\Palumbo et al. - 2008 - Huge Multidimensional Data Visualization Back to .pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Cartesian Space,Data Visualization,Exploratory Data Analysis,Stable Group,Statistical Unit},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{polzehlStructuralAdaptiveSmoothing2008,
  title = {Structural {{Adaptive Smoothing}} by {{Propagation}}–{{Separation Methods}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Polzehl, Jörg and Spokoiny, Vladimir},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {471--492},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_19},
  url = {https://doi.org/10.1007/978-3-540-33037-0_19},
  urldate = {2020-10-29},
  abstract = {Regression is commonly used to describe and analyze the relation between explanatory input variables X and one or multiple responses Y. In many applications such relations are too complicated to model with a parametric regression function. Classical nonparametric regression (see e.g., Fan and Gijbels, 1996;Wand and Jones, 1995; Loader, 1999; Simonoff, 1996) and varying coefficient models (see e.g., Hastie and Tibshirani, 1993; Fan and Zhang, 1999; Carroll et al., 1998; Cai et al., 2000), allow for a more flexible form. In this article we describe an approach that allows us to efficiently handle discontinuities and spatial inhomogeneities of the regression function in such models.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\P572KL6H\\Polzehl and Spokoiny - 2008 - Structural Adaptive Smoothing by Propagation–Separ.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Local Model,Mean Absolute Error,Mean Square Error,Noisy Image,Nonparametric Regression},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@book{R,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2020},
  location = {{Vienna, Austria}}
}

@report{raft91,
  title = {How Many Iterations in the {{Gibbs}} Sampler?},
  author = {Raftery, Adrian E and Lewis, Steven},
  date = {1991},
  institution = {{Washington University Seatle, Department of Statistics, United States}}
}

@article{ragh01,
  title = {A Multivariate Technique for Multiply Imputing Missing Values Using a Sequence of Regression Models},
  author = {Raghunathan, Trivellore E and Lepkowski, James M and Van Hoewyk, John and Solenberger, Peter},
  date = {2001},
  journaltitle = {Survey methodology},
  shortjournal = {Survey methodology},
  volume = {27},
  pages = {85--96},
  issn = {0714-0045},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\ZF27VSE7\\Raghunathan e.a. - 2001 - A multivariate technique for multiply imputing mis.pdf},
  number = {1}
}

@report{ragh07,
  title = {Diagnostics for {{Multiple Imputations}}},
  author = {Raghunathan, Trivellore and Bondarenko, Irina},
  date = {2007-11-21},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=1031750},
  urldate = {2019-10-17},
  abstract = {Multiple imputation technique is becoming a popular method for analyzing data with missing values. Several methods have been proposed for creating multiple imputations and most of these methods assume that the data are missing at random (MAR). However, limited diagnostic tools are available to check whether the imputations created by these methods are reasonable. This article develops a set of diagnostic tools based on certain conditional distributions of the observed and imputed values. These conditional distributions should be similar if the assumed model for creating multiple imputations is a good fit. The tools are formulated in terms of numerical summaries and graphical displays and could be easily implemented using the standard complete data software packages. For implementing these methods the exact nature of the model used by the imputer is not needed. The method is illustrated using a data set with large number of variables of different types with varying amount of missing values.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\WL7ZL76E\\Raghunathan en Bondarenko - 2007 - Diagnostics for Multiple Imputations.pdf},
  keywords = {Congeniality,Diagnostics,Missing at Random,Propensity score matching},
  langid = {english},
  number = {ID 1031750},
  type = {SSRN Scholarly Paper}
}

@book{rcoreteamLanguageEnvironmentStatistical2020,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2020},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}}
}

@article{rubin04,
  title = {The {{Design}} of a {{General}} and {{Flexible System}} for {{Handling Nonresponse}} in {{Sample Surveys}}},
  author = {Rubin, Donald B},
  date = {2004-11-01},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {58},
  pages = {298--302},
  doi = {10.1198/000313004X6355},
  url = {https://amstat.tandfonline.com/doi/abs/10.1198/000313004X6355},
  urldate = {2019-10-01},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\RJ2HAQHD\\Rubin - 2004 - The Design of a General and Flexible System for Ha.pdf;C\:\\Users\\4216318\\Zotero\\storage\\EN6WGCA4\\000313004X6355.html},
  number = {4}
}

@article{rubin76,
  title = {Inference and {{Missing Data}}},
  author = {Rubin, Donald B.},
  date = {1976},
  journaltitle = {Biometrika},
  volume = {63},
  pages = {581--592},
  doi = {10.2307/2335739},
  abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
  number = {3}
}

@book{rubin87,
  title = {Multiple {{Imputation}} for Nonresponse in Surveys},
  author = {Rubin, Donald B.},
  date = {1987},
  publisher = {{Wiley}},
  location = {{New York, NY}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\63UKR3PY\\Rubin - 1987 - Multiple Imputation for nonresponse in surveys.pdf},
  langid = {english},
  pagetotal = {258},
  series = {Wiley Series in Probability and Mathematical Statistics {{Applied}} Probability and Statistics}
}

@article{rubin96,
  title = {Multiple {{Imputation After}} 18+ {{Years}}},
  author = {Rubin, Donald B.},
  date = {1996},
  journaltitle = {Journal of the American Statistical Association},
  volume = {91},
  pages = {473--489},
  doi = {10.2307/2291635},
  abstract = {[Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies.]},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\9UVT28MF\\Rubin - Multiple Imputation after 18+ years.pdf},
  number = {434}
}

@article{sas14,
  title = {Sensitivity {{Analysis}} in {{Multiple Imputation}} for {{Missing Data}}},
  author = {Yuan, Yang},
  journaltitle = {In Proceedings of the SAS Global Forum 2014 Conference},
  abstract = {Multiple imputation, a popular strategy for dealing with missing values, usually assumes that the data are missing at random (MAR). That is, for a variable Y, the probability that an observation is missing depends only on the observed values of other variables, not on the unobserved values of Y. It is important to examine the sensitivity of inferences to departures from the MAR assumption, because this assumption cannot be verified using the data.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\NQKZC84X\\Yuan - Sensitivity Analysis in Multiple Imputation for Mi.pdf},
  langid = {english}
}

@article{scha02,
  title = {Missing Data: Our View of the State of the Art.},
  author = {Schafer, Joseph L and Graham, John W},
  date = {2002},
  journaltitle = {Psychological methods},
  shortjournal = {Psychological methods},
  volume = {7},
  pages = {147},
  issn = {1939-1463},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\RP4NJ9T2\\Schafer en Graham - 2002 - Missing data our view of the state of the art..pdf},
  number = {2}
}

@book{scha97,
  title = {Analysis of Incomplete Multivariate Data},
  author = {Schafer, Joseph L},
  date = {1997},
  publisher = {{Chapman and Hall/CRC}},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\Q7G6U2VV\\Schafer - 1997 - Analysis of incomplete multivariate data.pdf;C\:\\Users\\4216318\\Zotero\\storage\\VEI29NIR\\Schafer - 1997 - Analysis of incomplete multivariate data.pdf}
}

@article{schaferMultipleImputationMultivariate1998a,
  title = {Multiple {{Imputation}} for {{Multivariate Missing}}-{{Data Problems}}: {{A Data Analyst}}'s {{Perspective}}},
  author = {Schafer, Joseph L. and Olsen, Maren K.},
  date = {1998-10-01},
  journaltitle = {Multivariate Behavioral Research},
  shortjournal = {Multivariate Behavioral Research},
  volume = {33},
  pages = {545--571},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10.1207/s15327906mbr3304_5},
  url = {https://doi.org/10.1207/s15327906mbr3304_5},
  number = {4}
}

@article{schwartzDraftUserManual,
  title = {Draft User’s Manual. {{Proper}} Use of the {{Schwartz Value Survey}}},
  author = {Schwartz, Shalom H. and Littrell, Romie F.},
  journaltitle = {Unpublished manuscript, Auckland, New Zealand},
  url = {https://www.academia.edu/29396758/Draft_user_s_manual._Proper_use_of_the_Schwartz_Value_Survey},
  urldate = {2020-04-28},
  abstract = {Draft user’s manual. Proper use of the Schwartz Value Survey},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\H3VTECHE\\Draft_user_s_manual.html},
  langid = {english}
}

@incollection{shiehVisualizationGeneticNetwork2008,
  title = {Visualization for {{Genetic Network Reconstruction}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Shieh, Grace Shwu-Rong and Guo, Chin-Yuan},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {793--811},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_30},
  url = {https://doi.org/10.1007/978-3-540-33037-0_30},
  urldate = {2020-10-29},
  abstract = {This chapter reviews data visualization techniques that are used to reconstruct genetic networks from genomics data. Reconstructed genetic networks are predicted interactions among genes of interest and these interactions are inferred from genomics data, e.g., microarray data or/and DNA sequence data. Genomics data are generally contaminated and high-dimensional. The dimensionality of a microarray is the number of genes in it, and it usually numbers in the thousands at least. It is important to examine and clean data carefully to attain meaningful inferences. Thus visualization tools that are used in the preprocessing of data associated with genetic network reconstruction are also reviewed.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\HG3P8DTK\\Shieh and Guo - 2008 - Visualization for Genetic Network Reconstruction.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Gene Expression Time Series Data,Gene Pair,Genetic Interaction,Genetic Interaction Network,Genetic Network},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{shiny,
  title = {Shiny: Web Application Framework for {{R}}},
  author = {Chang, Winston and Cheng, Joe and Allaire, JJ and Xie, Yihui and McPherson, Jonathan},
  date = {2017},
  url = {https://CRAN.R-project.org/package=shiny}
}

@article{su11,
  title = {Multiple {{Imputation}} with {{Diagnostics}} (Mi) in {{R}}: {{Opening Windows}} into the {{Black Box}}},
  shorttitle = {Multiple {{Imputation}} with {{Diagnostics}} (Mi) in {{R}}},
  author = {Su, Yu-Sung and Gelman, Andrew E. and Hill, Jennifer and Yajima, Masanao},
  date = {2011},
  volume = {45},
  pages = {1--31},
  doi = {10.7916/D8VQ3CD3},
  url = {https://doi.org/10.7916/D8VQ3CD3},
  urldate = {2019-09-10},
  abstract = {Our mi package in R has several features that allow the user to get inside the imputation process and evaluate the reasonableness of the resulting models and imputations. These features include: choice of predictors, models, and transformations for chained imputation models; standard and binned residual plots for checking the fit of the conditional distributions used for imputation; and plots for comparing the distributions of observed and imputed data. In addition, we use Bayesian models and weakly informative prior distributions to construct more stable estimates of imputation models. Our goal is to have a demonstration package that (a) avoids many of the practical problems that arise with existing multivariate imputation programs, and (b) demonstrates state-of-the-art diagnostics that can be applied more generally and can be incorporated into the software of others.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\S5SZMIHW\\Su e.a. - 2011 - Multiple Imputation with Diagnostics (mi) in R Op.pdf;C\:\\Users\\4216318\\Zotero\\storage\\9KLRX5W9\\D8VQ3CD3.html},
  langid = {english},
  number = {2}
}

@incollection{symanzikInteractiveLinkedMicromap2008,
  title = {Interactive {{Linked Micromap Plots}} for the {{Display}} of {{Geographically Referenced Statistical Data}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Symanzik, Jürgen and Carr, Daniel B.},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {267--294},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_12},
  url = {https://doi.org/10.1007/978-3-540-33037-0_12},
  urldate = {2020-10-29},
  abstract = {Over the last decade, researchers have developed many improvements to make statistical graphics more accessible to the general public. These improvements include making statistical summaries more visual and providing more information at the same time. Research in this area involved converting statistical tables into plots (Carr, 1994; Carr and Nusser, 1995), new ways of displaying geographically referenced data (Carr et al., 1992), and, inparticular, the developmentof linkedmicromap(LM)plots, often simply called micromaps (Carr and Pierson, 1996; Carr et al., 1998, 2000a). LM plots, initially called map row plots as well as linked map-attribute graphics, were first presented in a poster session sponsored by the American Statistical Association (ASA) Section on Statistical Graphics at the 1996 Joint Statistical Meetings (Olsen et al., 1996).More details on the history of LMplots and their connection to other research can be found in these early references on micromaps. More recent references on LM plots (Carr et al., 2000b; Carr, 2001) focused on their use for communicating summary data from health and environmental studies.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\PCQIC6BX\\Symanzik and Carr - 2008 - Interactive Linked Micromap Plots for the Display .pdf},
  isbn = {978-3-540-33037-0},
  keywords = {American Statistical Association,Color Scheme,Comprehensive Cancer Control,National Cancer Institute,West Nile Virus},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{taka17,
  title = {Statistical {{Inference}} in {{Missing Data}} by {{MCMC}} and {{Non}}-{{MCMC Multiple Imputation Algorithms}}: {{Assessing}} the {{Effects}} of {{Between}}-{{Imputation Iterations}}},
  shorttitle = {Statistical {{Inference}} in {{Missing Data}} by {{MCMC}} and {{Non}}-{{MCMC Multiple Imputation Algorithms}}},
  author = {Takahashi, Masayoshi},
  date = {2017-07-28},
  journaltitle = {Data Science Journal},
  volume = {16},
  pages = {37},
  doi = {10.5334/dsj-2017-037},
  abstract = {Incomplete data are ubiquitous in social sciences; as a consequence, available data are inefficient (ineffective) and often biased. In the literature, multiple imputation is known to be the standard method to handle missing data. While the theory of multiple imputation has been known for decades, the implementation is difficult due to the complicated nature of random draws from the posterior distribution. Thus, there are several computational algorithms in software: Data Augmentation (DA), Fully Conditional Specification (FCS), and Expectation-Maximization with Bootstrapping (EMB). Although the literature is full of comparisons between joint modeling (DA, EMB) and conditional modeling (FCS), little is known about the relative superiority between the MCMC algorithms (DA, FCS) and the non-MCMC algorithm (EMB), where MCMC stands for Markov chain Monte Carlo. Based on simulation experiments, the current study contends that EMB is a confidence proper (confidence-supporting) multiple imputation algorithm without between-imputation iterations; thus, EMB is more user-friendly than DA and FCS.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\UC3SL5U6\\Takahashi - 2017 - Statistical Inference in Missing Data by MCMC and .pdf;C\:\\Users\\4216318\\Zotero\\storage\\UJTMEUVT\\dsj-2017-037.html},
  keywords = {Conditional modeling,Incomplete data,Joint modeling,Markov chain Monte Carlo,MCMC,Nonresponse},
  langid = {english}
}

@incollection{theusHighdimensionalDataVisualization2008,
  title = {High-Dimensional {{Data Visualization}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Theus, Martin},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {151--178},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_7},
  url = {https://doi.org/10.1007/978-3-540-33037-0_7},
  urldate = {2020-10-29},
  abstract = {One of the biggest challenges in data visualization is to find general representations of data that can display the multivariate structure of more than two variables. Several graphic types like mosaicplots, parallel coordinate plots, trellis displays, and the grand tour have been developed over the course of the last three decades. Each of these plots is introduced in a specific chapter of this handbook. This chapter will concentrate on investigating the strengths and weaknesses of these plots and techniques and contrast them in the light of data analysis problems. One very important issue is the aspect of interactivity. Except for trellis displays, all the above plots need interactive features to rise to their full power. Some, like the grand tour, are only defined by using dynamic graphics.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\RBEBWMTU\\Theus - 2008 - High-dimensional Data Visualization.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Brand Person,Common Scale,Conditioning Variable,Projection Pursuit,Scaling Option},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@online{TimeSeriesUnderstanding,
  title = {Time Series - {{Understanding}} This Acf Output},
  journaltitle = {Cross Validated},
  url = {https://stats.stackexchange.com/questions/81754/understanding-this-acf-output},
  urldate = {2020-04-17},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\VQQ2NBF3\\81764.html}
}

@incollection{unwinExploratoryGraphicsFinancial2008,
  title = {Exploratory {{Graphics}} of a {{Financial Dataset}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Unwin, Antony and Theus, Martin and Härdle, Wolfgang},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {831--852},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_32},
  url = {https://doi.org/10.1007/978-3-540-33037-0_32},
  urldate = {2020-10-29},
  abstract = {The first stages of any data analysis are to get to know the aims of the study and to get to know the data. In this study the main goal is to predict a company’s chances of going bankrupt based on its recent financial returns. In another chapter of the Handbook, some sophisticated prediction models based on support vector machines are discussed for a similar dataset. Here, visualization methods are used to explore the large dataset of American company accounts that was made available for predicting bankruptcy in order to get to know the data and to assess the quality of the dataset. This is an initial exploratory analysis that does not use any expert accounting knowledge.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\SPHJ7VPT\\Unwin et al. - 2008 - Exploratory Graphics of a Financial Dataset.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Current Asset,Exploratory Data Analysis,Financial Dataset,Interactive Graphic,Total Asset},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{unwinGoodGraphics2008,
  title = {Good {{Graphics}}?},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Unwin, Antony},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {57--78},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_3},
  url = {https://doi.org/10.1007/978-3-540-33037-0_3},
  urldate = {2020-10-29},
  abstract = {This chapter discusses drawing good graphics to visualize the information in data. Graphics have been used for a long time to present data. Figure 2.1 is a scanned image from Playfair’s Commercial and Political Atlas of 1801, reproduced in Playfair (2005). The fairly continuous increase of both imports and exports, and the fact that the balance was in favour of England from 1720 on, can be seen easily. Some improvements might be made, but overall the display is effective and well drawn.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\XH2EDNLT\\Unwin - 2008 - Good Graphics.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Engine Size,Exploratory Graphic,Personal Taste,Road Race,Small Multiple},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{unwinIntroduction2008,
  title = {Introduction},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Unwin, Antony and Chen, Chun-houh and Härdle, Wolfgang},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {3--12},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_1},
  url = {https://doi.org/10.1007/978-3-540-33037-0_1},
  urldate = {2020-10-29},
  abstract = {This book is the third volume of the Handbook of Computational Statistics and covers the field of data visualization. In line with the companion volumes, it contains a collection of chapters by experts in the field to present readers with an up-to-date and comprehensive overview of the state of the art.Data visualization is an active area of application and research, and this is a good time to gather together a summary of current knowledge.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\5MSSWRAU\\Unwin et al. - 2008 - Introduction.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Data Visualization,Exploratory Graphic,Good Graphic,Graphic Display,Presentation Graphic},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{urbanekVisualizingTreesForests2008,
  title = {Visualizing {{Trees}} and {{Forests}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Urbanek, Simon},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {243--264},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_11},
  url = {https://doi.org/10.1007/978-3-540-33037-0_11},
  urldate = {2020-10-29},
  abstract = {Tree-basedmodels provide an appealing alternative to conventional models formany reasons. They are more readily interpretable, can handle both continuous and categorical covariates, can accommodate data with missing values, provide an implicit variable selection, and model interactionswell. Most frequently used tree-basedmodels are classification, regression, and survival trees. Visualization is important in conjunction with treemodels because in their graphical formthey are easily interpretable even without special knowledge. Interpretation of decision trees displayed as a hierarchy of decision rules is highly intuitive.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\UV7WPXXH\\Urbanek - 2008 - Visualizing Trees and Forests.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Recursive Partitioning,Split Point,Split Variable,Terminal Node,Tree Model},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{vanbuurenMultipleImputationMissing1999,
  title = {Multiple Imputation of Missing Blood Pressure Covariates in Survival Analysis},
  author = {van Buuren, S. and Boshuizen, H. C. and Knook, D. L.},
  date = {1999-03-30},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {18},
  pages = {681--694},
  issn = {0277-6715},
  doi = {10.1002/(sici)1097-0258(19990330)18:6<681::aid-sim71>3.0.co;2-r},
  abstract = {This paper studies a non-response problem in survival analysis where the occurrence of missing data in the risk factor is related to mortality. In a study to determine the influence of blood pressure on survival in the very old (85+ years), blood pressure measurements are missing in about 12.5 per cent of the sample. The available data suggest that the process that created the missing data depends jointly on survival and the unknown blood pressure, thereby distorting the relation of interest. Multiple imputation is used to impute missing blood pressure and then analyse the data under a variety of non-response models. One special modelling problem is treated in detail; the construction of a predictive model for drawing imputations if the number of variables is large. Risk estimates for these data appear robust to even large departures from the simplest non-response model, and are similar to those derived under deletion of the incomplete records.},
  eprint = {10204197},
  eprinttype = {pmid},
  keywords = {Aged,Aged; 80 and over,Blood Pressure,Cohort Studies,Data Collection,Demography,Female,Humans,Male,Meta-Analysis as Topic,Survival Analysis},
  langid = {english},
  number = {6},
  options = {useprefix=true}
}

@article{vanginkelStandardizedRegressionCoefficients2020,
  title = {Standardized {{Regression Coefficients}} and {{Newly Proposed Estimators}} for \$\$\{\vphantom\}{{R}}\vphantom\{\}\^\{\{2\}\}\$\${{R2}} in {{Multiply Imputed Data}}},
  author = {van Ginkel, Joost R.},
  date = {2020-03-11},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  issn = {1860-0980},
  doi = {10.1007/s11336-020-09696-4},
  url = {https://doi.org/10.1007/s11336-020-09696-4},
  urldate = {2020-03-23},
  abstract = {Whenever statistical analyses are applied to multiply imputed datasets, specific formulas are needed to combine the results into one overall analysis, also called combination rules. In the context of regression analysis, combination rules for the unstandardized regression coefficients, the t-tests of the regression coefficients, and the F-tests for testing \$\$R\^\{2\}\$\$R2 for significance have long been established. However, there is still no general agreement on how to combine the point estimators of \$\$R\^\{2\}\$\$R2 in multiple regression applied to multiply imputed datasets. Additionally, no combination rules for standardized regression coefficients and their confidence intervals seem to have been developed at all. In the current article, two sets of combination rules for the standardized regression coefficients and their confidence intervals are proposed, and their statistical properties are discussed. Additionally, two improved point estimators of \$\$R\^\{2\}\$\$R2 in multiply imputed data are proposed, which in their computation use the pooled standardized regression coefficients. Simulations show that the proposed pooled standardized coefficients produce only small bias and that their 95\% confidence intervals produce coverage close to the theoretical 95\%. Furthermore, the simulations show that the newly proposed pooled estimates for \$\$R\^\{2\}\$\$R2 are less biased than two earlier proposed pooled estimates.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\Y76EU5BB\\van Ginkel - 2020 - Standardized Regression Coefficients and Newly Pro.pdf},
  langid = {english},
  options = {useprefix=true}
}

@online{vats18,
  title = {Revisiting the {{Gelman}}-{{Rubin Diagnostic}}},
  author = {Vats, Dootika and Knudson, Christina},
  date = {2018-12-21},
  url = {http://arxiv.org/abs/1812.09384},
  urldate = {2019-11-05},
  abstract = {Gelman and Rubin's (1992) convergence diagnostic is one of the most popular methods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the seminal paper, researchers have developed sophisticated methods of variance estimation for Monte Carlo averages. We show that this class of estimators find immediate use in the Gelman-Rubin statistic, a connection not established in the literature before. We incorporate these estimators to upgrade both the univariate and multivariate Gelman-Rubin statistics, leading to increased stability in MCMC termination time. An immediate advantage is that our new Gelman-Rubin statistic can be calculated for a single chain. In addition, we establish a relationship between the Gelman-Rubin statistic and effective sample size. Leveraging this relationship, we develop a principled cutoff criterion for the Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved diagnostic via examples.},
  archivePrefix = {arXiv},
  eprint = {1812.09384},
  eprinttype = {arxiv},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\4Z9GIUF9\\Vats en Knudson - 2018 - Revisiting the Gelman-Rubin Diagnostic.pdf;C\:\\Users\\4216318\\Zotero\\storage\\5WTJRDRB\\Vats en Knudson - 2018 - Revisiting the Gelman-Rubin Diagnostic.pdf;C\:\\Users\\4216318\\Zotero\\storage\\GLJJJXBA\\1812.html},
  keywords = {Statistics - Computation,Statistics - Methodology},
  primaryClass = {stat}
}

@article{veht19,
  title = {Rank-Normalization, Folding, and Localization: {{An}} Improved \{\$\textbackslash widehat\{\vphantom{\}\}}{{R}}\vphantom\{\}\$\vphantom\{\} for Assessing Convergence of {{MCMC}}},
  shorttitle = {Rank-Normalization, Folding, and Localization},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
  date = {2019-03-19},
  url = {http://arxiv.org/abs/1903.08008},
  urldate = {2019-09-16},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic \$\textbackslash widehat\{R\}\$ of Gelman and Rubin (1992) has serious flaws and we propose an alternative that fixes them. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give concrete recommendations for how these methods should be used in practice.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\UY8ZSDS6\\Vehtari e.a. - 2019 - Rank-normalization, folding, and localization An .pdf;C\:\\Users\\4216318\\Zotero\\storage\\SZQ7NZQE\\1903.html},
  keywords = {Statistics - Computation,Statistics - Methodology}
}

@article{vink14,
  title = {Pooling Multiple Imputations When the Sample Happens to Be the Population},
  author = {Vink, Gerko and van Buuren, Stef},
  date = {2014-09-30},
  url = {http://arxiv.org/abs/1409.8542},
  urldate = {2019-12-13},
  abstract = {Current pooling rules for multiply imputed data assume infinite populations. In some situations this assumption is not feasible as every unit in the population has been observed, potentially leading to over-covered population estimates. We simplify the existing pooling rules for situations where the sampling variance is not of interest. We compare these rules to the conventional pooling rules and demonstrate their use in a situation where there is no sampling variance. Using the standard pooling rules in situations where sampling variance should not be considered, leads to overestimation of the variance of the estimates of interest, especially when the amount of missingness is not very large. As a result, populations estimates are over-covered, which may lead to a loss of statistical power. We conclude that the theory of multiple imputation can be extended to the situation where the sample happens to be the population. The simplified pooling rules can be easily implemented to obtain valid inference in cases where we have observed essentially all units and in simulation studies addressing the missingness mechanism only.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\9TV8WDM5\\Vink en van Buuren - 2014 - Pooling multiple imputations when the sample happe.pdf;C\:\\Users\\4216318\\Zotero\\storage\\P6KYWI5H\\1409.html},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  options = {useprefix=true}
}

@article{vinkMultipleImputationSquared2013,
  title = {Multiple {{Imputation}} of {{Squared Terms}}},
  author = {Vink, Gerko and van Buuren, Stef},
  date = {2013-11-01},
  journaltitle = {Sociological Methods \& Research},
  shortjournal = {Sociological Methods \& Research},
  volume = {42},
  pages = {598--607},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124113502943},
  url = {https://doi.org/10.1177/0049124113502943},
  urldate = {2020-03-20},
  abstract = {We propose a new multiple imputation technique for imputing squares. Current methods yield either unbiased regression estimates or preserve data relations. No method, however, seems to deliver both, which limits researchers in the implementation of regression analysis in the presence of missing data. Besides, current methods only work under a missing completely at random (MCAR) mechanism. Our method for imputing squares uses a polynomial combination. The proposed method yields both unbiased regression estimates, while preserving the quadratic relations in the data for both missing at random and MCAR mechanisms.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\VBSZWGVA\\Vink en van Buuren - 2013 - Multiple Imputation of Squared Terms.pdf},
  langid = {english},
  number = {4},
  options = {useprefix=true}
}

@article{vinknd,
  title = {Towards a Standardized Evaluation of Multiple Imputation Routines},
  author = {Vink, Gerko},
  year = {\bibstring{nodate}},
  abstract = {Developing new imputation methodology has become a very active field. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In this paper I propose a move towards a standardized evaluation of imputation methods. To demonstrate the need for standardization, I highlight a set of potential pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. This may lead to suboptimal use of multiple imputation in practice. Additionally, I suggest a course of action for simulating and evaluating missing data problems.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\ZXGCXPP4\\Vink - Towards a standardized evaluation of multiple impu.pdf},
  langid = {english}
}

@incollection{wardMultivariateDataGlyphs2008,
  title = {Multivariate {{Data Glyphs}}: {{Principles}} and {{Practice}}},
  shorttitle = {Multivariate {{Data Glyphs}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Ward, Matthew O.},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {179--198},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_8},
  url = {https://doi.org/10.1007/978-3-540-33037-0_8},
  urldate = {2020-10-29},
  abstract = {In the context of data visualization, a glyph is a visual representation of a piece of data where the attributes of a graphical entity are dictated by one or more attributes of a data record. For example, the width and height of a box could be determined by a student’s score on the midterm and final exam for a course, while the box’s color might indicate the genderof the student.Thedefinitionabove is ratherbroad, as it can cover such visual elements as the markers in a scatterplot, the bars of a histogram, or even an entire line plot. However, a narrower definition would not be sufficient to capture the wide range of data visualization techniques that have been developed over the centuries that are termed glyphs.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\WTQQFYIH\\Ward - 2008 - Multivariate Data Glyphs Principles and Practice.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Data Visualization,IEEE Computer Society,IEEE Conference,Multivariate Data,Stat Assoc},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{whit11,
  title = {Multiple Imputation Using Chained Equations: {{Issues}} and Guidance for Practice},
  shorttitle = {Multiple Imputation Using Chained Equations},
  author = {White, Ian R. and Royston, Patrick and Wood, Angela M.},
  date = {2011-02-20},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {30},
  pages = {377--399},
  issn = {1097-0258},
  doi = {10.1002/sim.4067},
  abstract = {Multiple imputation by chained equations is a flexible and practical approach to handling missing data. We describe the principles of the method and show how to impute categorical and quantitative variables, including skewed variables. We give guidance on how to specify the imputation model and how many imputations are needed. We describe the practical analysis of multiply imputed data, including model building and model checking. We stress the limitations of the method and discuss the possible pitfalls. We illustrate the ideas using a data set in mental health, giving Stata code fragments.},
  eprint = {21225900},
  eprinttype = {pmid},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\L74SBSX2\\White e.a. - 2011 - Multiple imputation using chained equations Issue.pdf},
  keywords = {Adolescent,Adult,Aged,Cardiovascular Diseases,Cholesterol,Female,Humans,Lipoproteins; HDL,Mental Health,Middle Aged,Models; Statistical,Multicenter Studies as Topic,Young Adult},
  langid = {english},
  number = {4}
}

@book{WhiteNoiseForecasting,
  title = {2.9 {{White}} Noise | {{Forecasting}}: {{Principles}} and {{Practice}}},
  shorttitle = {2.9 {{White}} Noise | {{Forecasting}}},
  url = {https://Otexts.com/fpp2/},
  urldate = {2020-06-04},
  abstract = {2nd edition},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\9T77PMBZ\\wn.html}
}

@incollection{wilhelmLinkedViewsVisual2008,
  title = {Linked {{Views}} for {{Visual Exploration}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Wilhelm, Adalbert},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {199--215},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_9},
  url = {https://doi.org/10.1007/978-3-540-33037-0_9},
  urldate = {2020-10-29},
  abstract = {The basic problem in visualization still is the physical limitation of the 2-D presentation space of paper and computer screens. There are basically four approaches to addressing this problem and to overcoming the restrictions of two-dimensionality: 1. Create a virtual reality environment or a pseudo-3-D environment by rotation that is capable of portraying higher-dimensional data at least in a 3-D setting. 2. Project high-dimensional data onto a 2-D coordinate system by using a datareductionmethod such as principal component analysis, projection pursuit, multidimensional scaling, or correspondence analysis. 3. Use a nonorthogonal coordinate system such as parallel coordinates which is less restricted by the two-dimensionality of paper. 4. Link low-dimensional displays.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\RHZHRI6D\\Wilhelm - 2008 - Linked Views for Visual Exploration.pdf},
  isbn = {978-3-540-33037-0},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{wilkinsonGraphtheoreticGraphics2008,
  title = {Graph-Theoretic {{Graphics}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Wilkinson, Lelend},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {121--150},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_6},
  url = {https://doi.org/10.1007/978-3-540-33037-0_6},
  urldate = {2020-10-29},
  abstract = {This chapter will cover the uses of graphs for making graphs. This overloading of terms is an unfortunate historical circumstance that conflated graph-of-a-function usage with graph-of-vertices-and-edges usage. Vertex-edge graphs have long been understood as fundamental to the development of algorithms. It has become increasingly evident that vertex-edge graphs are also fundamental to the development of statistical graphics and visualizations.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\S4KU6RL3\\Wilkinson - 2008 - Graph-theoretic Graphics.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Delaunay Triangulation,Directed Acyclic Graph,Hierarchical Tree,Medial Axis,Span Tree},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{willsLinkedDataViews2008,
  title = {Linked {{Data Views}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Wills, Graham},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {217--241},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_10},
  url = {https://doi.org/10.1007/978-3-540-33037-0_10},
  urldate = {2020-10-29},
  abstract = {The linked views paradigm is a method of taking multiple simple views of data and allowing interactions with one to modify the display of data in all the linked views. A simple example is that selecting a data case in one view shows that data case highlighted in all other views. In this section we define the underlying methodology and show how it has been applied historically and how it can be extended to provide enhanced power. In particular we focus on displays of aggregated data and linking domain-specific views such as graph layouts and maps to statistical views.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\JZJGUCNH\\Wills - 2008 - Linked Data Views.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Data View,Dynamic Graphic,National League,Salary Data,Summary Function},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{wuMatrixVisualization2008,
  title = {Matrix {{Visualization}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Wu, Han-Ming and Tzeng, ShengLi and Chen, Chun-houh},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {681--708},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_26},
  url = {https://doi.org/10.1007/978-3-540-33037-0_26},
  urldate = {2020-10-29},
  abstract = {The graphical exploration of quantitative/qualitative data is an initial but essential step inmodern statistical data analysis.Matrix visualization (Chen, 2002; Chen et al., 2004) is a graphical technique that can simultaneously explore the associations between thousands of subjects, variables, and their interactions, without needing to first reduce the dimensions of the data. Matrix visualization involves permuting the rows and columns of the raw data matrix using suitable seriation (reordering) algorithms, together with the corresponding proximity matrices.The permuted raw data matrix and two proximity matrices are then displayed as matrix maps via suitable color spectra, and the subject clusters, variable groups, and interactions embedded in the dataset can be extracted visually.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\GZAEDCDB\\Wu et al. - 2008 - Matrix Visualization.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Binary Data Matrix,Exploratory Data Analysis,Hierarchical Cluster Tree,Proximity Matrix,Proximity Measure},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@incollection{yamamotoWebBasedStatisticalGraphics2008,
  title = {Web-{{Based Statistical Graphics}} Using {{XML Technologies}}},
  booktitle = {Handbook of {{Data Visualization}}},
  author = {Yamamoto, Yoshiro and Iizuka, Masaya and Fujino, Tomokazu},
  editor = {Chen, Chun-houh and Härdle, Wolfgang and Unwin, Antony},
  date = {2008},
  pages = {757--789},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0_29},
  url = {https://doi.org/10.1007/978-3-540-33037-0_29},
  urldate = {2020-10-29},
  abstract = {Most statistical graphics on theWeb are static, noninteractive and undynamic, even though other statistical analysis systems usually provide various interactive statistical graphics. Interactive and dynamic graphics, see Symanzik (2004), can be implemented using Internet technologies such as Java or Flash (Adobe, 2007). Scalable Vector Graphics (SVG) and Extensible 3D (X3D) offer alternative means of realizing an XML-based graphics format. One advantage of using XML is that data from a wide range of research topics are easy to deal with, because they are all presented in the XML format. Another advantage is that XML is a text-based graphics format, i.e., it is scriptable, meaning that it can be generated dynamically by a statistical analysis system or web application. Before introducing XML-based graphics, we introduce the relationship between theWeb, XML, and statistical graphics.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\9IRRRG2R\\Yamamoto et al. - 2008 - Web-Based Statistical Graphics using XML Technolog.pdf},
  isbn = {978-3-540-33037-0},
  keywords = {Dynamic Graphic,Mouse Cursor,Open Geospatial Consortium,Statistical Graphic,Vector Graphic},
  langid = {english},
  series = {Springer {{Handbooks Comp}}.{{Statistics}}}
}

@article{zhangSystematicSurveyReporting2017,
  title = {A Systematic Survey on Reporting and Methods for Handling Missing Participant Data for Continuous Outcomes in Randomized Controlled Trials},
  author = {Zhang, Yuqing and Flórez, Ivan D. and Colunga Lozano, Luis E. and Aloweni, Fazila Abu Bakar and Kennedy, Sean Alexander and Li, Aihua and Craigie, Samantha and Zhang, Shiyuan and Agarwal, Arnav and Lopes, Luciane C. and Devji, Tahira and Wiercioch, Wojtek and Riva, John J. and Wang, Mengxiao and Jin, Xuejing and Fei, Yutong and Alexander, Paul and Morgano, Gian Paolo and Zhang, Yuan and Carrasco-Labra, Alonso and Kahale, Lara A. and Akl, Elie A. and Schünemann, Holger J. and Thabane, Lehana and Guyatt, Gordon H.},
  date = {2017-08-01},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {Journal of Clinical Epidemiology},
  volume = {88},
  pages = {57--66},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2017.05.017},
  url = {http://www.sciencedirect.com/science/article/pii/S0895435617305735},
  urldate = {2020-09-07},
  abstract = {Objective To assess analytic approaches randomized controlled trial (RCT) authors use to address missing participant data (MPD) for patient-important continuous outcomes. Study Design and Setting We conducted a systematic survey of RCTs published in 2014 in the core clinical journals that reported at least one patient-important outcome analyzed as a continuous variable. Results Among 200 studies, 187 (93.5\%) trials explicitly reported whether MPD occurred. In the 163 (81.5\%) trials that reported the occurrence of MPD, the median and interquartile ranges of the percentage of participants with MPD were 11.4\% (2.5\%–22.6\%).Among the 147 trials in which authors made clear their analytical approach to MPD, the approaches chosen included available data only (109, 67\%); mixed-effect models (10, 6.1\%); multiple imputation (9, 4.5\%); and last observation carried forward (9, 4.5). Of the 163 studies reporting MPD, 16 (9.8\%) conducted sensitivity analyses examining the impact of the MPD and (18, 11.1\%) discussed the risk of bias associated with MPD. Conclusion RCTs reporting continuous outcomes typically have over 10\% of participant data missing. Most RCTs failed to use optimal analytic methods, and very few conducted sensitivity analyses addressing the possible impact of MPD or commented on how MPD might influence risk of bias.},
  keywords = {Analytic approaches,Continuous outcome,Lost to follow-up,Missing participant data,MPD,Randomized controlled trials},
  langid = {english}
}

@article{zhu15,
  title = {Convergence {{Properties}} of a {{Sequential Regression Multiple Imputation Algorithm}}},
  author = {Zhu, Jian and Raghunathan, Trivellore E.},
  date = {2015-07-03},
  journaltitle = {Journal of the American Statistical Association},
  volume = {110},
  pages = {1112--1124},
  doi = {10.1080/01621459.2014.948117},
  abstract = {A sequential regression or chained equations imputation approach uses a Gibbs sampling-type iterative algorithm that imputes the missing values using a sequence of conditional regression models. It is a flexible approach for handling different types of variables and complex data structures. Many simulation studies have shown that the multiple imputation inferences based on this procedure have desirable repeated sampling properties. However, a theoretical weakness of this approach is that the specification of a set of conditional regression models may not be compatible with a joint distribution of the variables being imputed. Hence, the convergence properties of the iterative algorithm are not well understood. This article develops conditions for convergence and assesses the properties of inferences from both compatible and incompatible sequence of regression models. The results are established for the missing data pattern where each subject may be missing a value on at most one variable. The sequence of regression models are assumed to be empirically good fit for the data chosen by the imputer based on appropriate model diagnostics. The results are used to develop criteria for the choice of regression models. Supplementary materials for this article are available online.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\FPPUB4TU\\Zhu en Raghunathan - 2015 - Convergence Properties of a Sequential Regression .pdf;C\:\\Users\\4216318\\Zotero\\storage\\CJWQJ3FF\\01621459.2014.html},
  keywords = {Bayesian analysis,Chained equations,Compatible conditionals,Conditional specifications,Exponential family,Gibbs sampling,Missing data.},
  number = {511}
}


