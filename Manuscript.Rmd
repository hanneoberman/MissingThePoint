---
title: "Missing The Point"
author: "Hanne Oberman"
date: "7-9-2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
load("~/MissingThePoint/Data/results_COV.Rdata")

```

# Missing The Point: Non-Convergence in Iterative Imputation Algorithms

## Abstract

Iterative imputation is a popular tool to accommodate the ubiquitous problem of missing data. While it is widely accepted that this technique can yield valid inferences, these inferences all rely on algorithmic convergence. Our study provides insight into identifying non-convergence in iterative imputation algorithms, since there is no consensus on how to evaluate the convergence properties currently. We found that--in the cases considered--inferential validity was achieved after five to ten iterations, much earlier than indicated by diagnostic methods. We conclude that it never hurts to iterate longer, but such calculations hardly bring added value.

## Intro

Missing data poses a ubiquitous threat against unbiased, confidence-valid statistical inferences. A popular tool to accommodate missing data is iterative imputation. With techniques like multiple imputation (Rubin, 1987), missing data points are imputed (i.e., 'filled in')

- Missing data has to be accommodated, usually filled in

- Filling in is algorithmic, which relies on convergence

- Diagnosing convergence is difficult, both visually and diagnostically

- In this paper we explore diagnostic methods for MICE, and show which parameter has the best performance --> most informative about convergence 


## Identifying non-convergence

- What does non-convergence look like? What are the consequences?

- 


## Simulation

- 3x3 missingness conditions: MCAR, MAR, MNAR + 5, 50, 95% of cases incomplete

- outcomes: estimated regression coefficient, CIW, coverage, R2 --> multivariate is key, because univariate convergence does not guarantee multivariate convergence

- convergence parameters: chain means, chain variances, scientific parameter, lambda

- diagnostics: rhat, ac

## Some results

```{r echo=FALSE}
results %>%
  ggplot() +
  #geom_hline(yintercept = 0.3334) +
  geom_point(aes(x=it, y=est, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=est, color = mech, linetype = as.factor(p))) +
  theme_classic()


```

Bias in estimate depends mostly on the amount of missingness (less info to estimate the relation??), but overall MNAR performs the worst, which is stable after it=5.

Update with different CI computation, see https://stefvanbuuren.name/fimd/sec-evaluation.html#sec:quantifyingbias

```{r}
results %>%
  ggplot() +
  #geom_hline(yintercept = 0.95) +
  geom_point(aes(x=it, y=cov, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=cov, color = mech, linetype = as.factor(p))) +
  theme_classic()


```

Cov is more or less fine, except for MNAR with only 0.05% incomplete cases --> due to bias in the estimate in combination with very narrow CI (see CIW!). However, all stable after it=3??

```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=CIW, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=CIW, color = mech, linetype = as.factor(p))) +
  theme_classic()


```

CIW clearly determined by the proportion of incomplete cases. Worst for p=0.95, then stable after it 7.

```{r}
results %>%
  ggplot() +
  #geom_hline(yintercept = 0.2090) +
  geom_point(aes(x=it, y=rsq, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=rsq, color = mech, linetype = as.factor(p))) +
  theme_classic()

```

Magnitude of the bias in R^2 perfectly follows the missingness mechanisms: least bias for MCAR, most for MNAR. Worst effect of non-convergence for p=0.95, MNAR, but stable from it=5.

```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=ac.max.mu.Y, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=ac.max.mu.Y, color = mech, linetype = as.factor(p))) +
  theme_classic()
```


```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=r.hat.max.mu.Y, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=r.hat.max.mu.Y, color = mech, linetype = as.factor(p))) +
  theme_classic()

```

```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=ac.max.sigma.Y, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=ac.max.sigma.Y, color = mech, linetype = as.factor(p))) +
  theme_classic()
```


```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=r.hat.max.sigma.Y, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=r.hat.max.sigma.Y, color = mech, linetype = as.factor(p))) +
  theme_classic()

```

```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=ac.max.qhat, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=ac.max.qhat, color = mech, linetype = as.factor(p))) +
  theme_classic()
```


```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=r.hat.max.qhat, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=r.hat.max.qhat, color = mech, linetype = as.factor(p))) +
  theme_classic()

```


```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=ac.max.lambda, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=ac.max.lambda, color = mech, linetype = as.factor(p))) +
  theme_classic()
```


```{r}
results %>%
  ggplot() +
  geom_point(aes(x=it, y=r.hat.max.lambda, color = mech, shape = as.factor(p))) +
  geom_line(aes(x=it, y=r.hat.max.lambda, color = mech, linetype = as.factor(p))) +
  theme_classic()

```

## Discussion

- Estimates etc do not improve after it = 7

- Convergence diagnostics keep improving, but are all stable after 20-30 it

- Univariate thetas do not differentiate between missingness mechanisms or proportions of incomplete cases

- Determining non-stationarity with lambda is more difficult than with qhat :(


