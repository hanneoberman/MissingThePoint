---
title: "Missing The Point"
author: "Hanne Oberman"
date: "7-9-2020"
output: html_document
---

```{r setup, include=FALSE}
# environment
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mice)
load("Data/results.Rdata")

# sim results plotting function
generic_plot <- function(v, name, ...){
  results %>%
    mutate(
      p = factor(p, levels = c(0.05, 0.5, 0.95), labels = c("5%", "50%", "95%")),
      mech = factor(mech, levels = c("MCAR", "MAR", "MNAR"))
      ) %>% 
  ggplot(aes(
    x=.data[["it"]], 
    y=.data[[v]], 
    color = .data[["p"]], 
    shape = .data[["mech"]], 
    linetype = .data[["mech"]]
    )) +
  geom_line(na.rm = TRUE) +
  labs(
    x = "Number of iterations",
    y = name,
    color = "Incomplete \n cases", 
    shape = "Missingness \n mechanism", 
    linetype = "Missingness \n mechanism") +
  theme_classic() + 
  scale_color_manual(values = c(
    #"#009E73", #green
    "#56B4E9", #blue
    #"#E69F00", #orange
    "#F0E442", #yellow
    "#D55E00"  #red
  ))
  # scale_color_manual(values = c(
  #   # '#228833', #green
  #   # '#66CCEE', #blue
  #   '#CCBB44', #yellow
  #   '#EE6677',  #red~ish
  #   '#AA3377'  #maroon
  #   ))
}
```

# Missing The Point: Non-Convergence in Iterative Imputation Algorithms

## Abstract

Iterative imputation is a popular tool to accommodate the ubiquitous problem of missing data. While it is widely accepted that this technique can yield valid inferences, these inferences all rely on algorithmic convergence. Our study provides insight into identifying non-convergence in iterative imputation algorithms, since there is no consensus on how to evaluate the convergence properties currently. We found that--in the cases considered--inferential validity was achieved after five to ten iterations, much earlier than indicated by diagnostic methods. We conclude that it never hurts to iterate longer, but such calculations hardly bring added value.

## Intro

Missing data pose a ubiquitous threat to anyone who aims to obtain unbiased, confidence-valid statistical inferences. A popular technique to accommodate missing data is to 'impute' (i.e., fill in) any missing values in an incomplete dataset. Imputation procedures like 'Multiple Imputation by Chained Equations' (MICE) have proven to be powerful tools to draw valid inferences under many missing data circumstances (Rubin, 1987; van Buuren, 2018). Most imputation software packages rely on iterative algorithms to obtain imputed values. 

With iterative imputation, the validity of the inference depends on the state-space of the algorithm at the final iteration. This introduces a potential threat to the validity of the imputations: What if the algorithm has not converged? Are the imputations then to be trusted? And can we rely on the inference obtained using the imputed data? These remain open questions since the convergence properties of iterative imputation algorithms have not been systematically studied (Van Buuren, 2018, 6.5.2). While there is no scientific consensus on how to evaluate the convergence of imputation algorithms (Zhu & Raghunathan, 2015; Takahashi, 2017), the current practice is to visually inspect imputations for signs of non-convergence. 

Identifying non-convergence through visual inspection may be undesirable for several reasons: 1) it may be challenging to the untrained eye, 2) only severely pathological cases of non-convergence may be diagnosed, and 3) there is not an objective measure that quantifies convergence (Van Buuren, 2018, 6.5.2). Therefore, a quantitative diagnostic method to identify non-convergence would be preferred.

In this paper we explore diagnostic methods for iterative imputation algorithms. For reasons of brevity, we focus on the iterative imputation algorithm implemented in the popular `mice` package (REF) in `R` (REF). We consider two tested non-convergence identifiers for MCMC algorithms: autocorrelation (conform Lynch, 2007, p. 147) and potential scale reduction factor $\widehat{R}$ (conform Vehtari et al., 2019, p. 5).  MICE, and show which method (parameter + diagnostic + threshold) is the most informative about convergence



- Missing data has to be accommodated, usually filled in

- Filling in is algorithmic, which relies on convergence

- Diagnosing convergence is difficult, both visually and diagnostically

- In this paper we explore diagnostic methods for MICE, and show which method (parameter + diagnostic + threshold) is the most informative about convergence 


## Identifying non-convergence

- What does non-convergence look like? What are the consequences? Refer to van Buuren (2018) instead of reproducing it. 

- 


## Simulation

- 3x3 missingness conditions: MCAR, MAR, MNAR + 5, 50, 95% of cases incomplete

- outcomes: estimated regression coefficient, CIW, coverage, R2 --> multivariate is key, because univariate convergence does not guarantee multivariate convergence

- convergence parameters: chain means, chain variances, scientific parameter, lambda

- diagnostics: rhat, ac

## Some results

```{r echo=FALSE}
generic_plot(v = "est", name = "Regression estimate") + 
  list(geom_hline(yintercept = 1, color = "grey"))

```

Bias in estimate depends mostly on the amount of missingness (less info to estimate the relation??), but overall MNAR performs the worst, which is stable after it=5.

Update with different CI computation, see https://stefvanbuuren.name/fimd/sec-evaluation.html#sec:quantifyingbias

```{r echo=FALSE}
generic_plot( v ="cov", name = "Coverage rate") + 
  list(geom_hline(yintercept = 0.95, color = "grey"))

# generic_plot( v ="cov", name = "Coverage rate") + 
#   list(geom_hline(yintercept = 0.95, color = "grey")) +
#   ylim(0.85, 1)

```

Cov is more or less fine, except for MNAR with only 0.05% incomplete cases --> due to bias in the estimate in combination with very narrow CI (see CIW!). However, all stable after it=3??

```{r echo=FALSE}
generic_plot(v="CIW", name = "CI width") 

```

CIW clearly determined by the proportion of incomplete cases. Worst for p=0.95, then stable after it 7.

```{r echo=FALSE}
generic_plot(v="rsq", name = "Coefficient of determination (r^2)") +
  list(geom_hline(yintercept = 0.5, color = "grey"))

```

Magnitude of the bias in r^2 perfectly follows the missingness mechanisms: least bias for MCAR, most for MNAR. Worst effect of non-convergence for p=0.95, MNAR, but stable from it=5.

```{r echo=FALSE}
generic_plot(v="ac.max.mu.Y", name = "AC chain means") 

```

This plot above and the next three plots show autocorrelations with different thetas

```{r echo=FALSE}
generic_plot(v="ac.max.sigma.Y", name = "AC chain variances") 

```

```{r echo=FALSE}
generic_plot(v="ac.max.qhat", name = "AC scientific estimate") 

```

```{r echo=FALSE}
generic_plot(v="ac.max.lambda", name = "AC novel theta") 

```

```{r echo=FALSE}
generic_plot(v="r.hat.max.mu.Y", name = "R hat chain means") 

```

This plot above and the next three plots show r hat values with different thetas

```{r echo=FALSE}
generic_plot(v="r.hat.max.sigma.Y", name = "R hat chain variances") 

```

```{r echo=FALSE}
generic_plot(v="r.hat.max.qhat", name = "R hat scientific estimate") 

```

```{r echo=FALSE}
generic_plot(v="r.hat.max.lambda", name = "R hat novel theta") 

```

These rhat plots all show some initialization before the fifth iteration: is rhat usefull before that??

## Discussion

- Estimates etc do not improve after it = 7

- Convergence diagnostics keep improving, but are all stable after 20-30 it

- Univariate thetas do not differentiate between missingness mechanisms or proportions of incomplete cases

- Determining non-stationarity with lambda is more difficult than with qhat :(

<!-- ## Check if heuristic for AC works -->
<!-- ```{r} -->
<!-- load("Data/diagnostics.Rdata") -->
<!-- load("Data/raw_outcomes.Rdata") -->
<!-- sim_nr <- 3 -->
<!-- outcomes[[sim_nr]] %>% filter(mech == "MCAR", p == 0.95) %>% ggplot(aes(x=it, y = est)) + geom_line() -->
<!-- convergence_diagnostics %>% filter(sim == sim_nr, mech == "MCAR", p ==0.95) %>% ggplot(aes(x=it, y = ac.max.qhat)) + geom_line() -->
<!-- convergence_diagnostics %>% filter(sim == sim_nr, mech == "MCAR", p ==0.95) %>% ggplot(aes(x=it, y = ac.max.lambda)) + geom_line() -->
<!-- ``` -->

methodological explanation is that rhat and ac have a lag (few it to inform your statistic) --> will always indicate conv slower than the est

calc rhat and cov for each block of 5 it

disc: conf val is likely to happen much sooner

as soon as we start downweighting the first few it from the calc, the memory effect would 

## Example data

```{r}
# load data
load("Data/example.Rdata")
load("Data/example_mids.Rdata")

# plot each var of example data
plotfun <- function(v, ...){
ggplot(example, aes(x = .data[["it"]], y = .data[[v]])) +
  geom_line() +
  theme_classic()
}

plotfun(v="est")
plotfun(v="ac.max.qhat") #+ list(scale_y_continuous(limits = c(0.65,1)))
plotfun(v="ac.max.lambda")
plotfun(v="r.hat.max.qhat")+list(geom_hline(yintercept = 1.2, color = "grey"), scale_y_continuous(limits = c(1,2)))
plotfun(v="r.hat.max.lambda")+list(geom_hline(yintercept = 1.2, color = "grey"), scale_y_continuous(limits = c(1,2)))

plot(mids)
```

