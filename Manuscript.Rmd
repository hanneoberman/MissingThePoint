---
title: "Missing The Point: Non-Convergence in Iterative Imputation Algorithms"
author: "Hanne Oberman"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  bookdown::pdf_document2
bibliography: ref.bib
---

```{r setup, include=FALSE}
# environment
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.width = 10)
library(tidyverse)
library(mice)
library(patchwork)
# load data
load("Data/parameters.Rdata")
load("Data/results_25_75.Rdata")
load("Data/example.Rdata")
load("Data/example_chains.Rdata")

# sim results plotting function
simulation_plot <- function(d = results, v, name, ...){
  d %>%
    mutate(
      p = factor(p, levels = c(0.25, 0.5, 0.75), labels = c("25%", "50%", "75%")),
      mech = factor(mech, levels = c("MCAR", "MAR", "MNAR")),
      est = est - 1,
      rsq = rsq - 0.5
      ) %>% 
  ggplot(aes(
    x=.data[["it"]], 
    y=.data[[v]], 
    color = .data[["p"]], 
    shape = .data[["mech"]], 
    linetype = .data[["mech"]]
    )) +
  geom_line(na.rm = TRUE) +
  labs(
    x = "Number of iterations",
    y = name,
    color = "Incomplete \n cases", 
    shape = "Missingness \n mechanism", 
    linetype = "Missingness \n mechanism") +
  theme_classic() + 
  scale_color_manual(values = c(
    #"#009E73", #green
    "#56B4E9", #blue
    #"#E69F00", #orange
    "#F0E442", #yellow
    "#D55E00"  #red
  )) 
  # scale_color_manual(values = c(
  #   # '#228833', #green
  #   # '#66CCEE', #blue
  #   '#CCBB44', #yellow
  #   '#EE6677',  #red~ish
  #   '#AA3377'  #maroon
  #   ))
}

# plot each var of example data
casestudy_plot <- function(d = example, v, name, ...){
ggplot(d, aes(x = .data[["it"]], y = .data[[v]])) +
  geom_line(na.rm = TRUE) +
    labs(x = "Iteration number",
         y = name) +
  theme_classic()
}

```



*Notation in this manuscript: *

- square brackets are comments for myself/stuff I need to review

- bullet points are things I need to expand

# Abstract

Iterative imputation has become the de facto standard to accommodate for the ubiquitous problem of missing data. While it is widely accepted that this technique can yield valid inferences, these inferences all rely on algorithmic convergence. Our study provides insight into identifying non-convergence in iterative imputation algorithms. We show that these algorithms can yield correct outcomes even when a converged state has not yet formally been reached. In the cases considered, inferential validity is achieved after five to ten iterations, much earlier than indicated by diagnostic methods. We conclude that it never hurts to iterate longer, but such calculations hardly bring added value.

# Intro

## One

- Iterative imputation has become the de facto standard to accommodate for missing data. 

- the aim is usually to draw valid inferences, i.e. to get 
<!-- ... the aim is to draw valid inference: to get  -->
unbiased, confidence-valid estimates that incorporate the effects of the missingness

- such estimates are obtained in it imp by separating the missing data problem from the scientific problem

- the missing values are imputed (i.e., filled in) using some sort of algorithm. And subsequently, the scientific model of int is performed on the completed data. 

<!-- data problem is solved first: by imputing (i.e., filling in) the missing values in the incomplete dataset, using an algorithm such as MICE -->

<!-- - then the analysis of scientific interest can be performed on the completed data, which yields the scientific estimates -->

- to obtain valid scientific estimates, both the mis dat prob and the sci prob should be considered appropriately. 
<!-- both the missing data problem and the scientific problem should be solved correctly -->

- the validity of this whole process naturally depends on the convergence of the algorithm that was used to to generate the imp. 
<!-- one aspect that is often overlooked is the convergence of the algorithm that is used to generate imputations -->


## Two

- determining whether an alg has conv is not trivial. 
<!-- there is no consensus on the convergence properties of iterative imputation algorithms, and  -->
there has not been a systematic study on how to evaluate the convergence of it imp algorithms

- yet, all inferences rely on the convergence of such alg

- a widely accepted practice is visual inspection, but identifying convergence through visual inspection may be undesirable for several reasons: 1) it may be challenging to the untrained eye, 2) only severely pathological cases of non-convergence may be diagnosed, and 3) there is not an objective measure that quantifies convergence (Van Buuren, 2018, 6.5.2). Therefore, a quantitative diagnostic method to identify non-convergence would be preferred.

<!-- this may be inadequate because of reasons 1, 2, and 3 [see original text] -->
## new alinea

- it is challenging to arrive at a single point at which conv has been reached. the algo may prod some fluc even after it has conv. the aim is to conv to a distr and not to a single point.

<!-- - it is difficult that there is not at single point at which convergence is reached: the algorithm will produce some fluctuations even after convergence [it's convergence in distribution, not to a point]  <!-- (and a ... distribution may not exist because of univariate modeling of multivariate distribution???) --> -->

- because of this property, it may be more desirable to focus on non-c.

<!-- therefore, we should look at *non*-convergence --> [discussie: als we geen non-conv kunnen observeren, moeten we ons dan zorgen maken om convergentie??] -->

- fortunately there are non-convergence identifiers for other iterative algorithms. but the validity of these identifiers has not been syst eval on imp alg.
<!-- (MCMC), but it is not known whether these are appropriate for imputation algorithms as well -->


## Three 

- in this study, we explore different methods for identifying non-convergence in iterative imputation algorithms. we eval whether these are able to cover the extend of the non-c but we also invest the relation between non-c and the validity of the inf.

we investigate whether these identifiers are able to 


- since the ultimate goal is to have inferential validity, we use this as performance measure in a simulation study: if the estimates are unbiased and confidence-valid, the non-convergence is, apparently, not influential

- we develop guidelines for practice to interpret the non-convergence identifiers, because we typically don't have the ground truth (to calculate bias and coverage) 

- we demonst the workings of all this by means of a case study

<!-- Iterative imputation has become the de facto standard approach to accommodate missing data problems. With iterative imputation, we 'impute' (i.e., fill in) plausible values in an incomplete dataset. These plausible values are generated using algorithms such as 'MICE' [@mice]. These algorithms sequence through the variables in an incomplete dataset  -->

<!-- Iterative imputation entails using algorithms such as MICE [@mice] to replace missing values by plausible values. These plausible values are sampled from the distribution of each missing entry, and draw a plausible value from this distribution to replace the missing value by. -->

<!-- While it is widely accepted that the technique can yield valid inferences under many different missing data circumstances, these inferences all rely on algorithmic convergence. We fill in the missing data by modeling a multivariate distribution for each missing entry. We update dis model by sequencing through the variables See Figure 1. Van Buuren et al. (2006) have shown that just a few iterations maybe sufficient. Most imputation software packages use a default value of five or ten iterations. But how can we know did this is sufficient?  -->
<!-- There is currently no consensus on how to evaluate the convergence properties of iterative imputation algorithms. Imputation algorithms such as mice are a type of Gibbs sampler. It would does make sense to evaluate the convergence of iterative imputation algorithms using common MSMC non convergence identifiers. This has not been done.  -->
<!-- With MCMC algorithms there is not a single point at which convergence is reached.  -->

## Original text [remove later]

Iterative imputation has become the de facto standard approach to accommodate missing data. With iterative imputation, we 'impute' (i.e., fill in) plausible data values in the place of missing values in an incomplete dataset. We obtain these plausible values by sampling from a distribution that we model for each missing entry and update sequentially. @buur06 have shown that with an iterative imputation algorithm such as 'MICE', just a few iterations are enough to yield unbiased, confidence-valid statistical inferences. 

<!-- Missing data pose a ubiquitous threat to anyone who aims to obtain unbiased, confidence-valid statistical inferences. The de facto approach to accommodate missing data is to 'impute' (i.e., fill in) any missing entries in an incomplete data. With iterative imputation, we model the distribution of each missing entry, and draw one or more plausible values from it.  -->

<!-- With iterative imputation, we 'impute' (i.e., fill in) the missing data in an incomplete dataset by drawing values from the distribution of each missing entry. To generate imputed values, most imputation software packages rely on iterative algorithms. [Why iterative? We estimate the posterior predictive distribution of the missing values; our estimates become better as we go; there should be a point at which our estimates are non-improving.]  -->

Iterative imputation techniques have proven to be powerful tools to draw valid inference under many missing data circumstances [@rubin87; @buur18]. 

With iterative imputation, the validity of the inference depends on the state-space of the algorithm at the final iteration. This introduces a potential threat to the validity of the imputations: What if the algorithm has not converged? Are the imputations then to be trusted? And can we rely on the inference obtained using the imputed data? These remain open questions since the convergence properties of iterative imputation algorithms have not been systematically studied [@buur18]. [Common convergence diagnostics may not work.] While there is no scientific consensus on how to evaluate the convergence of imputation algorithms [@liu14; @zhu15; @taka17], the current practice is to visually inspect imputations for signs of non-convergence. 
<!-- (Van Buuren, 2018, 6.5.2) (Zhu & Raghunathan, 2015; Takahashi, 2017) -->

Identifying non-convergence through visual inspection may be undesirable for several reasons: 1) it may be challenging to the untrained eye, 2) only severely pathological cases of non-convergence may be diagnosed, and 3) there is not an objective measure that quantifies convergence (Van Buuren, 2018, 6.5.2). Therefore, a quantitative diagnostic method to identify non-convergence would be preferred.

In this paper, we explore some existing and new means of diagnosing non-convergence in iterative algorithms. We evaluate the validity of these methods in the context of missing data imputation using model-based simulation in `R` [@R]. [Brief overview of paper: the diagnostics, the simulation, the results, the discussion, a case study.]

<!-- In this paper, we explore diagnostic methods for iterative imputation algorithms. For reasons of brevity, we focus on the iterative imputation algorithm implemented in the popular `mice` package [@mice] in `R` [@R]. We consider two non-convergence identifiers for iterative algorithms: autocorrelation (conform Lynch, 2007, p. 147) and potential scale reduction factor $\widehat{R}$ (conform Vehtari et al., 2019, p. 5). Aside from the usual parameters to monitor---chain means and chain variances---we also investigate convergence in the multivariate state-space of the algorithm. -->

<!-- We propose a novel multivariate parameter to check for non-convergence in iterative algorithms. We aim to show which method is the most informative about non-convergence in iterative imputation [explain that method = parameter + diagnostic + interpretation]. -->

<!-- - Missing data has to be accommodated, usually filled in -->

<!-- - Filling in is algorithmic, which relies on convergence -->

<!-- - Diagnosing convergence is difficult, both visually and diagnostically -->

<!-- - In this paper we explore diagnostic methods for MICE, and show which method (parameter + diagnostic + threshold) is the most informative about convergence  -->

# Identifying non-convergence [remove later]

- Iterative imputation is a sort of MCMC algorithm, so it makes sense to investigate non-convergence in a similar fashion to typical MCMC algorithms. 

- With MCMC, the generated values will vary even after convergence, so there is not a unique point at which convergence is established [@gelm13]. Therefore, diagnostic methods may only identify signs of *non*-convergence [@hoff09]. Non-convergence occurs when one of two requirements for convergence is not met.

- There are two requirements for convergence of iterative algorithms: mixing and
stationarity [@gelm13]. Mixing implies that generated values intermingle nicely, and stationarity is characterized by the absence of trending between successive draws. 

- What are the consequences? --\> bias, under-coverage [refer to van Buuren (2018) instead of reproducing this]

<!-- -   Existing non-convergence identifiers for MCMC algorithms are not all applicable, e.g. N\_eff is estimated using the variance-covariance matrix of the set of samples, while this is not the case in iterative imputation per se. -->

- We consider autocorrelation and rhat, and monitor four parameters: chain means, chain variances, a scientific estimate, and lambda.

- Explain lambda

```{r eval=FALSE, include=FALSE}
# # maybe add ESS instead of AC???
# LaplacesDemon::ESS(parameters[parameters$m == 1 & parameters$sim == 1, "qhat"])
```


# Simulation Set-Up

<!-- The aim of the simulation study is to determine the impact of non-convergence on the validity of statistical inferences, and to assess whether non-convergence may be detected using several diagnostic methods. Inferential validity is reached when estimates are both unbiased and have nominal coverage across simulation repetitions (nsim = 1000). -->

We investigate non-convergence in iterative imputation through model-based simulation in R (version 4.0.3; R Core Team 2020). We provide a summary of the simulation set-up in Algorithm 1; the complete script and technical details are available from [github.com/hanneoberman/MissingThePoint](https://github.com/hanneoberman/MissingThePoint).

**Algorithm 1: simulation set-up (pseudo-code)**

    for each simulation repetition 
      1. simulate complete data
      for each missingness condition 
        2. create missingness
        for each iteration 
          3. impute missingness
          4. perform analysis of scientific interest
          5. apply non-convergence identifiers
          6. pool results across imputations
          7. compute performance measures
        8. combine outcomes from all iterations
      9. combine outcomes from all missingness conditions
    10. aggregate outcomes from all simulation repetitions

## Aims

With this simulation, we assess the impact of non-convergence on the validity of scientific estimates obtained using the imputation package `{mice}` [@mice]. Inferential validity is reached when estimates are both unbiased and have nominal coverage across simulation repetitions ($n_{sim} = 1000$). To induce non-convergence, we terminate the algorithm after a varying number of iterations ($n_{it} = 1, 2, ..., 50$). We differentiate between nine different missingness scenarios that are defined by the data generating mechanism. 
<!-- : three missingness mechanisms versus three proportions of incomplete cases [remove here if already defined above/below??]. -->

## Data generating mechanism

Data are generated in each simulation repetition for a complete set of $n_{obs} = 1000$ cases (i.e., before inducing missingness). We define three multivariately normal random variables, let

$$
\begin{pmatrix}
Y\\
X_1\\
X_2
\end{pmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix},
\begin{pmatrix}
2     &       &  \\
0.5   & 1     &  \\
-0.5  & 0.5   & 1
\end{pmatrix}
\end{bmatrix}.
$$ 

The complete set is amputed according to nine missingness conditions. We use a $3 \times 3$ factorial design consisting of three missingness mechanisms and three proportions of incomplete cases. 

- we use the three missingness mechanisms MCAR, MAR, and MNAR with defaults settings from `mice::ampute()` (e.g., right-tailed MAR)

- we use a multivariate missing data pattern and make 25%, 50%, and 75% of cases incomplete

## Estimands

We impute the missing data five times ($m = 5$) using Bayesian linear regression imputation with `mice` [@mice]. On each imputed dataset, we perform multiple linear regression to predict outcome variable $Y$ from the other two variables

$$
\hat{Y} = \beta_0 + \beta_1 X_1 + \beta_2 X_2,
$$

where $\hat{Y}$ is the predicted value of the outcome. Our estimands are the regression coefficient $\beta_1$ and coefficient of determination $\rho^2$ that we obtain after pooling the regression results across the imputations.

## Methods

- non-convergence in iterative algorithms is diagnosed using an identifier and a parameter

- the parameter can be any statistic that we track across iterations, for example chain means (i.e, the average imputed value per imputation)

- the identifier is a calculation of some sort that quantifies non-convergence

- identifiers are historically focused on either non-mixing between chains or non-stationarity within chains

- the popular non-mixing identifier rhat has recently been updated by Vehtari et al, and should now work for non-stationarity as well

- just to be sure, we also use autocorrelation to quantify trending within chains

- we apply these identifiers to four different univariate and multivariate parameters 

- the univariate parameters are those commonly used for visual inspection: chain means and chain variances

- one multivariate parameter that is of immediate interest for our estimand is the estimated value itself: regression coefficient beta_1

- we also propose a new multivariate parameter that is not dependent on the model of scientific interest: the first eigenvalue of the variance-covariance matrix of the completed data [explain!]

We use eight different methods to diagnose non-convergence: a combination of two non-convergence identifiers---autocorrelation and $\widehat{R}$---and four parameters---chain means, chain variances, $\beta_1$, and $\lambda$.

## Performance measures

As recommended by @buur18, our performance measures are bias, confidence interval width, and coverage rate of the estimands ($\S$ 2.5.2)[or just of the regression estimate?? otherwise add the ciw and cov of r2 too!!].

# Simulation Results

The following figures display the simulation results for the eight diagnostic methods and four performance measures, contrasted to the number of iterations in the imputation algorithm. Within the figures, we split the results according to the missingness conditions [missingness mechanisms as line types, and proportion of incomplete cases as colors]. Note that these results are averages of the $n_{sim} = 1000$ simulation repetitions.

## Diagnostic Methods

Figure \@ref(fig:ac) shows the autocorrelations in the chain means (panel A), chain variances (panel B), regression estimates (panel C), and eigenvalues (panel D).
<!-- The next four plots show the autocorrelations in each parameter plotted against the number of iterations. -->

```{r ac, fig.cap = "Autocorrelation (AC) with different parameters.", echo=FALSE}
ac_means <- simulation_plot(v="ac.max.mu.Y", name = "AC chain means") + ggplot2::scale_y_continuous(limits = c(0.175, 1))
ac_vars <- simulation_plot(v="ac.max.sigma.Y", name = "AC chain variances") + ggplot2::scale_y_continuous(limits = c(0.175, 1))
ac_betas <- simulation_plot(v="ac.max.qhat", name = quote("AC"~beta)) + ggplot2::scale_y_continuous(limits = c(0.175, 1))
ac_lambdas <- simulation_plot(v="ac.max.lambda", name = quote("AC"~lambda)) + ggplot2::scale_y_continuous(limits = c(0.175, 1))
(ac_means + ac_vars) / (ac_betas + ac_lambdas) + plot_layout(guides="collect") + plot_annotation(tag_levels = 'A')

```

Autocorrelation in the chain means decreases rapidly in the first few iterations (see \@ref(fig:ac)A). The decrease is substantive until $n_{it} \geq 6$. This means that there is some initial trending within chains, but the average imputed value quickly reaches stationarity. These results hold irrespective of the missingness condition.


Autocorrelation in the chain variances show us something similar (see \@ref(fig:ac)B). The number of iterations that is required to reach non-improving autocorrelations is somewhat more ambiguous than for chain means, but generally around $n_{it} \geq 10$. We do not observe a systematic difference between missingness conditions here either. 


There is more autocorrelation in the scientific estimates than in the chain means and chain variances (see \@ref(fig:ac)C). We observe the highest autocorrelations in conditions where 75% of cases are incomplete. Overall, the autocorrelations reach a plateau when $n_{it} \geq 20$ to $30$. There is no clear effect of the missingness mechanisms. 


The autocorrelation in the eigenvalues exhibits a similar trend to the autocorrelation in the scientific estimates (see \@ref(fig:ac)D). Trending in this parameter diminishes when $n_{it} \geq 20$.

Figure \@ref(fig:rh) shows the potential scale reduction factor in the chain means (panel A), chain variances (panel B), regression estimates (panel C), and eigenvalues (panel D).
<!-- The next four plots show the $\widehat{R}$ in the different parameters. -->

```{r rh, fig.cap = "Potential scale reduction factor (Rh) with different parameters.", echo=FALSE}
rh_means <- simulation_plot(v="r.hat.max.mu.Y", name = "Rh chain means") + ggplot2::scale_y_continuous(limits = c(1, 1.65))
rh_vars <- simulation_plot(v="r.hat.max.sigma.Y", name = "Rh chain variances") + ggplot2::scale_y_continuous(limits = c(1, 1.65))
rh_betas <- simulation_plot(v="r.hat.max.qhat", name = quote("Rh"~beta)) + ggplot2::scale_y_continuous(limits = c(1, 1.65))
rh_lambdas <- simulation_plot(v="r.hat.max.lambda", name = quote("Rh"~lambda)) + ggplot2::scale_y_continuous(limits = c(1, 1.65))
(rh_means + rh_vars) / (rh_betas + rh_lambdas) + plot_layout(guides="collect") + plot_annotation(tag_levels = 'A')

```


We observe that $\widehat{R}$-values of the chain means generally decreases as a function of the number of iterations (see \@ref(fig:rh)A). An exception to this observation is the initial increase when $3 \leq n_{it} \leq 5$ [interpret?? due to initialization or is there really more mixing initially??]. After the first couple of iterations, the mixing between chain means generally improves until $n_{it} \geq 30$ to $40$. There is no apparent differentiation between the missingness conditions. 


The mixing between chain variances mimics the mixing between chain means almost perfectly (see \@ref(fig:rh)B). Irrespective of the missingness condition, the $\widehat{R}$-values taper off around $n_{it} \geq 30$. 


With the regression estimate as parameter we observe very similar $\widehat{R}$-values than with chain means and chain variances (see \@ref(fig:rh)C). We do, however, see some differences between missingness conditions. Conditions where 75% of the cases are incomplete show more extreme non-mixing. The overall trend remains the same: about 30 iterations are required before mixing stops improving substantially.


$\widehat{R}$-values of the eigenvalues show a trend similar to the chain means and chain variances (see \@ref(fig:rh)D). [add something about conditions??]

[These rhat plots all show some initialization before the fifth iteration: is rhat useful before that??]

## Performance Measures

In Figure \@ref(fig:perf) we show the performance measures: bias in the regression estimate (panel A), bias in the coefficient of determination (panel B), the empirical coverage rate of the regression estimate (panel C) and the average confidence interval width of this estimate (panel D).

```{r perf, fig.cap = "Performance measures.", echo=FALSE}
est <- simulation_plot(v = "est", name = "Bias in regression estimate") + 
  list(geom_hline(yintercept = 0, color = "grey")) 
cov <- simulation_plot( v ="cov", name = "Coverage rate") + 
  list(geom_hline(yintercept = 0.95, color = "grey"))
ciw <- simulation_plot(v="CIW", name = "Average CI width") 
rsq <- simulation_plot(v="rsq", name = "Bias in coefficient of determination") +
  list(geom_hline(yintercept = 0, color = "grey"))
(est + rsq) / (cov + ciw) + plot_layout(guides="collect") + plot_annotation(tag_levels = 'A')
```

We see that within a few iterations the bias in the regression estimate approaches zero (see \@ref(fig:perf)A). When $n_{it} \geq 6$, even the worst-performing conditions (e.g., with a proportion of incomplete cases of 75%) produce stable, non-improving estimates [regression coefficient is underestimated because there is less info to estimate the relation??]. 

Equivalent to the bias in the regression estimate, the bias in the coefficient of determination tapers off within a couple of iterations (see \@ref(fig:perf)B). We observe stable estimates in all conditions when $n_{it} \geq 6$ [interpret the over-estimation in MNAR+75% condition?].

<!-- Update with different CI computation, see <https://stefvanbuuren.name/fimd/sec-evaluation.html#sec:quantifyingbias> -->

Nominal coverage is quickly reached (see \@ref(fig:perf)C). After just three iterations, the coverage rates are non-improving in every missingness condition [but MNAR with 5% incomplete cases does not reach nominal coverage --\> due to bias in the estimate in combination with very narrow CI (see CIW!)].


The average confidence interval width decreases quickly with every added iteration until a stable plateau is reached (see \@ref(fig:perf)D). Depending on the proportion of incomplete cases this takes up-to $n_{it} \geq 9$. 



<!-- Magnitude of the bias in $r^2$ perfectly follows the missingness mechanisms: least bias for MCAR, most for MNAR. Worst effect of non-convergence for $p=0.95$, MNAR, but stable from it=6. -->


# Discussion

Our study found that---in the cases considered---inferential validity was achieved after five to ten iterations, much earlier than indicated by the non-convergence identifiers. Of course, it never hurts to iterate longer, but such calculations hardly bring added value.


- Convergence diagnostics keep improving substantially until n_it = 20-30

- Performance measures do not improve after n_it = 9

- [methodological explanation is that rhat and ac have a lag (few it to inform your statistic) --\> will always indicate convergence slower than inferential validity is reached]

- Univariate thetas may under-estimate non-convergence. 

- Determining non-stationarity with lambda is more difficult than with qhat :(

<!-- # Check if heuristic for AC works -->

<!-- ```{r} -->

<!-- load("Data/diagnostics.Rdata") -->

<!-- load("Data/raw_outcomes.Rdata") -->

<!-- sim_nr <- 3 -->

<!-- outcomes[[sim_nr]] %>% filter(mech == "MCAR", p == 0.95) %>% ggplot(aes(x=it, y = est)) + geom_line() -->

<!-- convergence_diagnostics %>% filter(sim == sim_nr, mech == "MCAR", p ==0.95) %>% ggplot(aes(x=it, y = ac.max.qhat)) + geom_line() -->

<!-- convergence_diagnostics %>% filter(sim == sim_nr, mech == "MCAR", p ==0.95) %>% ggplot(aes(x=it, y = ac.max.lambda)) + geom_line() -->

<!-- ``` -->


In short, the validity of iterative imputation stands or falls with algorithmic convergence---or so it's thought. We have shown that iterative imputation algorithms can yield correct outcomes even when a converged state has not yet formally been reached. Any further iterations just burn computational resources without improving the statistical inferences. 

# Case Study

- We use real data: the `boys` dataset from the `mice` package

- We are interested in predicting age from the other variables, in particular in the regression coefficient of `hgt`

- We compare non-convergence identified using visual inspection versus rhat in the chain variances, scientific estimate and lambda.

- The figures show results of a `mice` run with 20 iterations but otherwise default settings.

```{r case, fig.cap = "Case study.", echo=FALSE}
trace <- param %>% 
  ggplot() + 
  geom_line(aes(x = it, y = mu.hgt, color = as.factor(m))) + 
  theme_classic() + 
  labs(x = "Iteration number",
       y = "Chain mean variable 'hgt'",
       color = "Imputation")
means <- casestudy_plot(v="r.hat.max.mu.hgt", name = "Rhat chain means") + 
  list(geom_hline(yintercept = 1.2, color = "grey"))
qhats <-
  casestudy_plot(v = "r.hat.max.qhat", name = "Rhat regression estimate") +
  list(geom_hline(yintercept = 1.2, color = "grey"))
lambdas <- casestudy_plot(v = "r.hat.max.lambda", name = "Rhat lambda") +
  list(geom_hline(yintercept = 1.2, color = "grey"))
(trace + means) / (qhats + lambdas) + plot_layout(guides="collect") + plot_annotation(tag_levels = 'A')

```

From the traceplot of the chain means (see \@ref(fig:case)A) it seems that mixing improves up-to 10 iterations, while trending is only apparent in the first three iterations. 



This figure (\@ref(fig:case)B) shows that 7 iterations are required before the $\widehat{R}$-values of the chain means drop below the threshold for non-convergence.


The $\widehat{R}$-values for the scientific estimate reaches the threshold much sooner, when $n_{it}=14$ (see \@ref(fig:case)C).


According to the $\widehat{R}$-values with $\lambda$ as parameter, at least 15 iterations are required (see \@ref(fig:case)D).


# References 
